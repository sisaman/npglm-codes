\documentclass[]{article}
\usepackage[backend=bibtex]{biblatex}
\usepackage{amsmath}
\addbibresource{References.bib}
%opening
\title{NP-GLM Method for Time-Aware Link Prediction}
\author{Sina Sajadmanesh}

\begin{document}

\maketitle

\section{Introduction}
In this paper we propose a novel method to predict the link formation times in networks. Our method, called NP-GLM, is an extension to the previous method, GLM. As opposed to GLM in which we need to specify the exact probability distribution of link creation times, our method learns the underlying distribution in a non-parametric fashion.

\section{Problem Formulation}
Given the feature vector $x_l$ for a missing link $l$ extracted in time $t_f$, we want to predict $t_l$ which shows how long after $t_f$ the link $l$ will appear in the network. To this end, we need to model $P(t_l\mid x_l)$, or better to say $f_T(t_l\mid x_l)$ if $t_l$ is continues.

Suppose that we have seen the network in the time interval $[t_0,t_e]$. if we set the feature extraction time $t_f$ sometime between $t_0$ and $t_e$, then:

\begin{enumerate}
\item some links are already present at time $t_f$.
\item some links are missing at $t_f$, but will appear after it before $t_e$.
\item some links are missing at $t_f$ and remain missing until $t_e$. 
\end{enumerate}

The 2nd and 3rd type of links form our data samples. For these links, we extract their feature vectors at time $t_f$. For a link $l$ of the 2nd type, we have seen it is created at a time like $t_c\in[t_f,t_e]$. So we set the $t_l=t_c-t_f$ as the time it takes for it to appear after $t_f$, and $y_l=1$ which indicates that we have seen its exact creation time. If $l$ is of the 3rd type, we haven't observed its exact creation time, but we know it is definitely after $t_e$. In this case, we set $t_l=t_e-t_0$ and $y_l=0$ to indicate that the recorded time is not exact. This way, each link $l$ is associated with a triple $(x_l,y_l,t_l)$ representing its feature vector, its observation status, and the time it takes to appear, respectively.

\section{Prior Work}
The only prior work have used Generalized Linear Model (GLM) to model $f_T(t_l\mid x_l)$. For this purpose, they assumed that the times come from a Weibull distribution. Therefore, they assume that $E[t_l]=\exp(w^Tx_l)$, where $w$ is the weight vector which is the same for all links. Since $E[t_l]$ is a function of parameters of the Weibull distribution, the whole distribution can be rewritten with respect to $w$, and then $w$ can be estimated using maximum likelihood. For more details, refer to \cite{sun2012will}.

\section{Proposed Method}
A shortcoming of the GLM is that we need to exactly specify the underlying distribution of times. We come over this problem by learning the distribution from the data using a non-parametric solution.

Since every distribution can be uniquely specified by its intensity function, we find a way to model the intensity function to eventually model the distribution itself. The intensity function of a distribution $f_T(t\mid x)$ is formally defined as:
\[\lambda(t\mid x)=\frac{f_T(t\mid x)}{S(t\mid x)} \]
where $S(t\mid x)=1-F(t\mid x)$ is the survival function of the distribution. Here, we hypothesize that $\lambda(t\mid x)$ which is a function of both $t$ and $x$, can be factorized into two functions as the following:
\[\lambda(t\mid x)=g(w^Tx)h(t)\]
where $g$ and $h$ are functions of just $x$ and $h$, respectively. If we fix $g$ and learn the function $h$ and the vector $w$ from the data, we have learned the whole distribution. If we have $N$ data samples, the likelihood of the data becomes:

\begin{equation*}
\begin{split}
\mathcal{L}(D)=&\prod_{i=1}^{N}f_T(t_i\mid x_i)^{y_i}P(T\ge t_i\mid x_i)^{1-y_i}\\
=&\prod_{i=1}^{N}\left[g(w^Tx_i)h(t_i)\right]^{y_i}\exp\lbrace-g(w^Tx_i)\int_{0}^{t_i}h(t)dt\rbrace
\end{split}
\end{equation*}

To deal with the integral in above equation, assuming $h(t)$ is a piecewise function which changes just in $t_i$ s, the integral becomes a series:
\[\int_{0}^{t_i}h(t)dt = \sum_{j=1}^{i-1}h(t_j)(t_{j+1}-t_j)=H(t_i)\]
assuming $t_i$ s are sorted in increasing order. Replacing the above series in the likelihood, and setting its derivative with respect to $h(t_k)$ to zero, yields a closed form equation for $h$:
\[h(t_k)=\frac{y_k}{(t_{k+1}-t_k)\sum_{i=k+1}^{N}g(w^Tx_i)}\]

To optimize the whole likelihood with respect to both $w$ and $h$, we use the coordinate ascend algorithm. Starting with a random $w$, we calculate $h(t_k)$ in each iteration using the above formula and replace it in the likelihood. Then, we perform optimization with respect to $w$. This procedure continues until convergence. If we set $g(w^tx)=\exp(w^Tx)$, then the negative log-likelihood will become a convex function of $w$, making the optimization more faster. With having $w$ and $h$ at hand, we can derive the distribution as the following equation:
\[f_T(t\mid x)=g(w^Tx)h(t)\exp(-g(w^Tx)H(t))\]
\printbibliography
\end{document}
