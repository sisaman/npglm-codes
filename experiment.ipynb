{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Apps\\Python36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pickle\n",
    "import threading\n",
    "import numpy as np\n",
    "from progressbar import *\n",
    "from models import Model\n",
    "from models.ExpGlm import ExpGlm\n",
    "from models.WblGlm import WblGlm\n",
    "from models.NpGlm import NpGlm\n",
    "from models.RayGlm import RayGlm\n",
    "from features.delicious.extraction import run as delicious_run\n",
    "from features.movielens.extraction import run as movielens_run\n",
    "from features.dblp.extraction import run as dblp_run\n",
    "from features.utils import timestamp_delta_generator\n",
    "from features.autoencoder import encode\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_squared_log_error, median_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(dist):\n",
    "    return {\n",
    "        'np': NpGlm(),\n",
    "        'wbl': WblGlm(),\n",
    "        'exp': ExpGlm(),\n",
    "        'ray': RayGlm(),\n",
    "#         'pow': PowGlm(),\n",
    "#         'gom': GomGlm()\n",
    "    }[dist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_c_index(T_true, T_pred, Y):\n",
    "    total_number_of_pairs = 0\n",
    "    number_of_correct_predictions = 0\n",
    "\n",
    "    for i in range(len(T_true)):\n",
    "        for j in range(len(T_true) - 1, i, -1):\n",
    "            if Y[i] != 0 or Y[j] != 0:  # if one or both of the samples are in observation window\n",
    "                total_number_of_pairs += 1\n",
    "                if T_true[i] > T_true[j] and T_pred[i] > T_pred[j]:\n",
    "                    number_of_correct_predictions += 1\n",
    "                if T_true[i] < T_true[j] and T_pred[i] < T_pred[j]:\n",
    "                    number_of_correct_predictions += 1\n",
    "                if T_true[i] == T_true[j] and T_pred[i] == T_pred[j]:\n",
    "                    number_of_correct_predictions += 1\n",
    "\n",
    "    return number_of_correct_predictions / total_number_of_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(X, Y, T, convert_to_month=False):\n",
    "    T = T.astype(np.float64)\n",
    "    if convert_to_month:\n",
    "        T /= timestamp_delta_generator(months=1)\n",
    "    T += np.random.rand(len(T)) * Y\n",
    "\n",
    "    index = np.argsort(T, axis=0).ravel()\n",
    "    X = X[index, :]\n",
    "    Y = Y[index]\n",
    "    T = T[index]\n",
    "\n",
    "    return X, Y, T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: Model, X_train: np.ndarray, Y_train: np.ndarray, T_train: np.ndarray, X_test: np.ndarray,\n",
    "             Y_test: np.ndarray, T_test: np.ndarray, acc_thresholds):\n",
    "    model.fit(X_train, Y_train, T_train)\n",
    "\n",
    "    # T_pred = model.mean(X_test)\n",
    "    T_pred = model.quantile(X_test, .5).ravel()\n",
    "    #     T_pred = np.fmin(T_pred, max(T_test))\n",
    "\n",
    "    c_index = generate_c_index(T_test, np.fmin(T_pred, max(T_test)), Y_test)\n",
    "\n",
    "    k = Y_test.sum()\n",
    "    # X_test = X_test[:k, :]\n",
    "    T_test = T_test[:k]\n",
    "    T_pred = T_pred[:k]\n",
    "\n",
    "    res = np.abs(T_pred - T_test)\n",
    "\n",
    "    distance = np.zeros((len(acc_thresholds)))\n",
    "    for i in range(len(acc_thresholds)):\n",
    "        distance[i] = (res <= acc_thresholds[i]).sum() / len(res)\n",
    "\n",
    "    #     ev = explained_variance_score(T_test, T_pred)\n",
    "    mae = mean_absolute_error(T_test, T_pred)\n",
    "    rmse = mean_squared_error(T_test, T_pred) ** .5\n",
    "    msle = mean_squared_log_error(T_test, T_pred)\n",
    "    mre = (res / T_test).mean()\n",
    "    mad = median_absolute_error(T_test, T_pred)\n",
    "    #     r2 = r2_score(T_test, T_pred)\n",
    "\n",
    "    return (mae, mre, rmse, msle, mad, c_index) + tuple(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(dists, X_stat, X, Y, T, cv=5, acc_thresholds:\n",
    "    threads = []\n",
    "    results = {dist+pos: [] for dist in dists for pos in ['', '_stat']}\n",
    "    k_fold = StratifiedKFold(n_splits=cv, shuffle=True)\n",
    "\n",
    "    widget = [Bar('=', '[', ']'), ' ', Percentage()]\n",
    "    bar = ProgressBar(maxval=cv*len(dists)*2, widgets=widget)\n",
    "    \n",
    "    for training_indices, test_indices in k_fold.split(X=X, y=Y):\n",
    "        X_stat_train = X_stat[training_indices, :]\n",
    "        X_train = X[training_indices, :]\n",
    "        Y_train = Y[training_indices]\n",
    "        T_train = T[training_indices]\n",
    "\n",
    "        X_stat_test = X_stat[test_indices, :]\n",
    "        X_test = X[test_indices, :]\n",
    "        Y_test = Y[test_indices]\n",
    "        T_test = T[test_indices]\n",
    "\n",
    "        def worker():\n",
    "            for dist in dists:\n",
    "                model = get_model(dist)\n",
    "                scores = evaluate(model, X_train, Y_train, T_train, X_test, Y_test, T_test, acc_thresholds)\n",
    "                results[dist].append(scores)\n",
    "                bar.update(bar.currval+1)\n",
    "                scores_stat = evaluate(model, X_stat_train, Y_train, T_train, X_stat_test, Y_test, T_test, acc_thresholds)\n",
    "                results[dist+'_stat'].append(scores_stat)\n",
    "                bar.update(bar.currval+1)\n",
    "\n",
    "        job = threading.Thread(target=worker)\n",
    "        threads.append(job)\n",
    "        \n",
    "    bar.start()\n",
    "\n",
    "    for t in threads:\n",
    "        t.start()\n",
    "    for t in threads:\n",
    "        t.join()\n",
    "    \n",
    "    bar.finish()\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_name(dist):\n",
    "    return {\n",
    "        'np': 'NP-Glm',\n",
    "        'wbl': 'Wbl-Glm',\n",
    "        'exp': 'Exp-Glm',\n",
    "        'ray': 'Ray-Glm',\n",
    "        'gom': 'Gom-Glm'\n",
    "    }[dist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:27:27: generating papers ...\n",
      "20:27:55: parsing dataset ...\n",
      "20:27:55: generating samples ...\n",
      "20:27:55: extracting ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:28:03: parsing dataset ...\n",
      "20:28:03: extracting ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============2009=============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:28:11: parsing dataset ...\n",
      "20:28:11: extracting ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============2008=============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:28:17: parsing dataset ...\n",
      "20:28:17: extracting ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============2007=============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:28:22: parsing dataset ...\n",
      "20:28:23: extracting ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============2006=============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:28:27: parsing dataset ...\n",
      "20:28:27: extracting ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============2005=============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:28:32: parsing dataset ...\n",
      "20:28:32: extracting ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============2004=============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:28:36: parsing dataset ...\n",
      "20:28:36: extracting ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============2003=============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:28:39: parsing dataset ...\n",
      "20:28:39: extracting ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============2002=============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:28:43: parsing dataset ...\n",
      "20:28:43: extracting ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============2001=============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:28:47: done.\n"
     ]
    }
   ],
   "source": [
    "X_list, Y_raw, T_raw = dblp_run(delta=1, observation_window=6, n_snapshots=9)\n",
    "# X_list, Y_raw, T_raw = delicious_run(delta=1, observation_window=6, n_snapshots=9)\n",
    "# X_list, Y_raw, T_raw = movielens_run(delta=1, observation_window=6, n_snapshots=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Apps\\Python36\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "limit = 4000\n",
    "if len(Y_raw) > limit:\n",
    "    X = np.stack(X_list, axis=1)  # X.shape = (n_samples, timesteps, n_features)\n",
    "    X, _, Y_raw, _, T_raw, _ = train_test_split(X, Y_raw, T_raw, train_size=limit, stratify=Y_raw, shuffle=True)\n",
    "    for i in range(len(X_list)):\n",
    "        X_list[i] = X[:,i,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "4000/4000 [==============================] - 2s 440us/step - loss: 3.9808\n",
      "Epoch 2/100\n",
      "4000/4000 [==============================] - 1s 188us/step - loss: 3.5266\n",
      "Epoch 3/100\n",
      "4000/4000 [==============================] - 1s 186us/step - loss: 3.4604\n",
      "Epoch 4/100\n",
      "4000/4000 [==============================] - 1s 188us/step - loss: 3.4271\n",
      "Epoch 5/100\n",
      "4000/4000 [==============================] - 1s 185us/step - loss: 3.4127\n",
      "Epoch 6/100\n",
      "4000/4000 [==============================] - 1s 182us/step - loss: 3.4063\n",
      "Epoch 7/100\n",
      "4000/4000 [==============================] - 1s 188us/step - loss: 3.4025\n",
      "Epoch 8/100\n",
      "4000/4000 [==============================] - 1s 182us/step - loss: 3.3999\n",
      "Epoch 9/100\n",
      "4000/4000 [==============================] - 1s 190us/step - loss: 3.3978\n",
      "Epoch 10/100\n",
      "4000/4000 [==============================] - 1s 182us/step - loss: 3.3961\n",
      "Epoch 11/100\n",
      "4000/4000 [==============================] - 1s 187us/step - loss: 3.3947\n",
      "Epoch 12/100\n",
      "4000/4000 [==============================] - 1s 183us/step - loss: 3.3936\n",
      "Epoch 13/100\n",
      "4000/4000 [==============================] - 1s 185us/step - loss: 3.3928\n",
      "Epoch 14/100\n",
      "4000/4000 [==============================] - 1s 191us/step - loss: 3.3922\n",
      "Epoch 15/100\n",
      "4000/4000 [==============================] - 1s 193us/step - loss: 3.3918\n",
      "Epoch 16/100\n",
      "4000/4000 [==============================] - 1s 201us/step - loss: 3.3916\n",
      "Epoch 17/100\n",
      "4000/4000 [==============================] - 1s 220us/step - loss: 3.3914\n",
      "Epoch 18/100\n",
      "4000/4000 [==============================] - 1s 188us/step - loss: 3.3913\n",
      "Epoch 19/100\n",
      "4000/4000 [==============================] - 1s 187us/step - loss: 3.3912\n",
      "Epoch 20/100\n",
      "4000/4000 [==============================] - 1s 190us/step - loss: 3.3911\n",
      "Epoch 21/100\n",
      "4000/4000 [==============================] - 1s 189us/step - loss: 3.3911\n",
      "Epoch 22/100\n",
      "4000/4000 [==============================] - 1s 192us/step - loss: 3.3910\n",
      "Epoch 23/100\n",
      "4000/4000 [==============================] - 1s 194us/step - loss: 3.3910\n",
      "Epoch 24/100\n",
      "4000/4000 [==============================] - 1s 225us/step - loss: 3.3909\n",
      "Epoch 25/100\n",
      "4000/4000 [==============================] - 1s 209us/step - loss: 3.3909\n",
      "Epoch 26/100\n",
      "4000/4000 [==============================] - 1s 216us/step - loss: 3.3909\n",
      "Epoch 27/100\n",
      "4000/4000 [==============================] - 1s 190us/step - loss: 3.3908\n",
      "Epoch 28/100\n",
      "4000/4000 [==============================] - 1s 186us/step - loss: 3.3908\n",
      "Epoch 29/100\n",
      "4000/4000 [==============================] - 1s 196us/step - loss: 3.3908\n",
      "Epoch 30/100\n",
      "4000/4000 [==============================] - 1s 190us/step - loss: 3.3907 0s - loss: 3.39\n",
      "Epoch 31/100\n",
      "4000/4000 [==============================] - 1s 195us/step - loss: 3.3907\n",
      "Epoch 32/100\n",
      "4000/4000 [==============================] - 1s 197us/step - loss: 3.3906\n",
      "Epoch 33/100\n",
      "4000/4000 [==============================] - 1s 197us/step - loss: 3.3906\n",
      "Epoch 34/100\n",
      "4000/4000 [==============================] - 1s 201us/step - loss: 3.3906\n",
      "Epoch 35/100\n",
      "4000/4000 [==============================] - 1s 241us/step - loss: 3.3905\n",
      "Epoch 36/100\n",
      "4000/4000 [==============================] - 1s 220us/step - loss: 3.3905\n",
      "Epoch 37/100\n",
      "4000/4000 [==============================] - 1s 205us/step - loss: 3.3904\n",
      "Epoch 38/100\n",
      "4000/4000 [==============================] - 1s 201us/step - loss: 3.3904\n",
      "Epoch 39/100\n",
      "4000/4000 [==============================] - 1s 208us/step - loss: 3.3903\n",
      "Epoch 40/100\n",
      "4000/4000 [==============================] - 1s 184us/step - loss: 3.3903\n",
      "Epoch 41/100\n",
      "4000/4000 [==============================] - 1s 199us/step - loss: 3.3902\n",
      "Epoch 42/100\n",
      "4000/4000 [==============================] - 1s 192us/step - loss: 3.3902\n",
      "Epoch 43/100\n",
      "4000/4000 [==============================] - 1s 191us/step - loss: 3.3901\n",
      "Epoch 44/100\n",
      "4000/4000 [==============================] - 1s 188us/step - loss: 3.3900\n",
      "Epoch 45/100\n",
      "4000/4000 [==============================] - 1s 188us/step - loss: 3.3899\n",
      "Epoch 46/100\n",
      "4000/4000 [==============================] - 1s 193us/step - loss: 3.3899\n",
      "Epoch 47/100\n",
      "4000/4000 [==============================] - 1s 197us/step - loss: 3.3898\n",
      "Epoch 48/100\n",
      "4000/4000 [==============================] - 1s 198us/step - loss: 3.3897\n",
      "Epoch 49/100\n",
      "4000/4000 [==============================] - 1s 219us/step - loss: 3.3896\n",
      "Epoch 50/100\n",
      "4000/4000 [==============================] - 1s 228us/step - loss: 3.3895\n",
      "Epoch 51/100\n",
      "4000/4000 [==============================] - 1s 223us/step - loss: 3.3894\n",
      "Epoch 52/100\n",
      "4000/4000 [==============================] - 1s 227us/step - loss: 3.3893\n",
      "Epoch 53/100\n",
      "4000/4000 [==============================] - 1s 229us/step - loss: 3.3892\n",
      "Epoch 54/100\n",
      "4000/4000 [==============================] - 1s 334us/step - loss: 3.3890\n",
      "Epoch 55/100\n",
      "4000/4000 [==============================] - 1s 223us/step - loss: 3.3889\n",
      "Epoch 56/100\n",
      "4000/4000 [==============================] - 1s 228us/step - loss: 3.3887\n",
      "Epoch 57/100\n",
      "4000/4000 [==============================] - 1s 228us/step - loss: 3.3885\n",
      "Epoch 58/100\n",
      "4000/4000 [==============================] - 1s 229us/step - loss: 3.3884\n",
      "Epoch 59/100\n",
      "4000/4000 [==============================] - 1s 221us/step - loss: 3.3882\n",
      "Epoch 60/100\n",
      "4000/4000 [==============================] - 1s 226us/step - loss: 3.3880 0s - los\n",
      "Epoch 61/100\n",
      "4000/4000 [==============================] - 1s 225us/step - loss: 3.3878\n",
      "Epoch 62/100\n",
      "4000/4000 [==============================] - 1s 229us/step - loss: 3.3876\n",
      "Epoch 63/100\n",
      "4000/4000 [==============================] - 1s 228us/step - loss: 3.3874\n",
      "Epoch 64/100\n",
      "4000/4000 [==============================] - 1s 228us/step - loss: 3.3871\n",
      "Epoch 65/100\n",
      "4000/4000 [==============================] - 1s 226us/step - loss: 3.3868\n",
      "Epoch 66/100\n",
      "4000/4000 [==============================] - 1s 229us/step - loss: 3.3865\n",
      "Epoch 67/100\n",
      "4000/4000 [==============================] - 1s 229us/step - loss: 3.3862\n",
      "Epoch 68/100\n",
      "4000/4000 [==============================] - 1s 229us/step - loss: 3.3859\n",
      "Epoch 69/100\n",
      "4000/4000 [==============================] - 1s 226us/step - loss: 3.3856\n",
      "Epoch 70/100\n",
      "4000/4000 [==============================] - 1s 231us/step - loss: 3.3852\n",
      "Epoch 71/100\n",
      "4000/4000 [==============================] - 1s 233us/step - loss: 3.3847\n",
      "Epoch 72/100\n",
      "4000/4000 [==============================] - 1s 228us/step - loss: 3.3842\n",
      "Epoch 73/100\n",
      "4000/4000 [==============================] - 1s 231us/step - loss: 3.3837\n",
      "Epoch 74/100\n",
      "4000/4000 [==============================] - 1s 229us/step - loss: 3.3831\n",
      "Epoch 75/100\n",
      "4000/4000 [==============================] - 1s 230us/step - loss: 3.3825\n",
      "Epoch 76/100\n",
      "4000/4000 [==============================] - 1s 224us/step - loss: 3.3819\n",
      "Epoch 77/100\n",
      "4000/4000 [==============================] - 1s 236us/step - loss: 3.3812\n",
      "Epoch 78/100\n",
      "4000/4000 [==============================] - 1s 236us/step - loss: 3.3805\n",
      "Epoch 79/100\n",
      "4000/4000 [==============================] - 1s 232us/step - loss: 3.3797\n",
      "Epoch 80/100\n",
      "4000/4000 [==============================] - 1s 227us/step - loss: 3.3788\n",
      "Epoch 81/100\n",
      "4000/4000 [==============================] - 1s 274us/step - loss: 3.3779\n",
      "Epoch 82/100\n",
      "4000/4000 [==============================] - 1s 278us/step - loss: 3.3772\n",
      "Epoch 83/100\n",
      "4000/4000 [==============================] - 1s 270us/step - loss: 3.3762\n",
      "Epoch 84/100\n",
      "4000/4000 [==============================] - 1s 249us/step - loss: 3.3747\n",
      "Epoch 85/100\n",
      "4000/4000 [==============================] - 1s 243us/step - loss: 3.3729\n",
      "Epoch 86/100\n",
      "4000/4000 [==============================] - 1s 232us/step - loss: 3.3713\n",
      "Epoch 87/100\n",
      "4000/4000 [==============================] - 1s 247us/step - loss: 3.3692\n",
      "Epoch 88/100\n",
      "4000/4000 [==============================] - 1s 252us/step - loss: 3.3668\n",
      "Epoch 89/100\n",
      "4000/4000 [==============================] - 1s 233us/step - loss: 3.3632\n",
      "Epoch 90/100\n",
      "4000/4000 [==============================] - 1s 251us/step - loss: 3.3582\n",
      "Epoch 91/100\n",
      "4000/4000 [==============================] - 1s 233us/step - loss: 3.3520\n",
      "Epoch 92/100\n",
      "4000/4000 [==============================] - 1s 257us/step - loss: 3.3400\n",
      "Epoch 93/100\n",
      "4000/4000 [==============================] - 1s 222us/step - loss: 3.3167 0s - \n",
      "Epoch 94/100\n",
      "4000/4000 [==============================] - 1s 203us/step - loss: 3.2217\n",
      "Epoch 95/100\n",
      "4000/4000 [==============================] - 1s 189us/step - loss: 3.1389\n",
      "Epoch 96/100\n",
      "4000/4000 [==============================] - 1s 195us/step - loss: 3.1089\n",
      "Epoch 97/100\n",
      "4000/4000 [==============================] - 1s 215us/step - loss: 3.0942\n",
      "Epoch 98/100\n",
      "4000/4000 [==============================] - 1s 207us/step - loss: 3.0885\n",
      "Epoch 99/100\n",
      "4000/4000 [==============================] - 1s 203us/step - loss: 3.0858\n",
      "Epoch 100/100\n",
      "4000/4000 [==============================] - 1s 203us/step - loss: 3.0842\n",
      "Autoencoder Training Loss: 3.0842\n"
     ]
    }
   ],
   "source": [
    "X_raw = encode(X_list, epochs=100, latent_factor=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[====================================================================    ]  95%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 117.1062262058258 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================================] 100%\r"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "X, Y, T = prepare_data(X_raw, Y_raw, T_raw)\n",
    "scaler = MinMaxScaler(copy=True)\n",
    "X_stat = scaler.fit_transform(X_list[0])\n",
    "\n",
    "dists = [\n",
    "    'np',\n",
    "    'wbl',\n",
    "#     'exp',\n",
    "#     'ray',\n",
    "    # 'gom'\n",
    "]\n",
    "\n",
    "print(len(T))\n",
    "\n",
    "results = cross_validate(dists, X_stat, X, Y, T, cv=5)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                MAE    MRE    RMSE    MSLE    MDAE    CI    ACC-1    ACC-2    ACC-3    ACC-4    ACC-5    ACC-6\n",
      "------------  -----  -----  ------  ------  ------  ----  -------  -------  -------  -------  -------  -------\n",
      "NP-Glm         1.65   0.81    2.02    0.24    1.47  0.70     0.17     0.34     0.51     0.66     0.77     0.86\n",
      "Wbl-Glm        3.57   2.02   25.43    0.44    1.63  0.70     0.16     0.30     0.45     0.59     0.69     0.78\n",
      "NP-Glm_stat    2.86   1.42    3.13    0.46    3.08  0.40     0.05     0.10     0.18     0.28     0.38     0.47\n",
      "Wbl-Glm_stat   2.89   1.43    3.17    0.47    3.05  0.50     0.04     0.10     0.18     0.28     0.36     0.48\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "table = []\n",
    "row = []\n",
    "header = ['MAE', 'MRE', 'RMSE', 'MSLE', 'MDAE', 'CI', 'ACC-1', 'ACC-2', 'ACC-3', 'ACC-4', 'ACC-5', 'ACC-6']\n",
    "for pos in ['', '_stat']:\n",
    "    for dist in dists:\n",
    "        row.append(get_name(dist)+pos)\n",
    "        result = np.array(results[dist+pos])\n",
    "        mean = result.mean(axis=0)\n",
    "        table.append(mean)\n",
    "print(tabulate(table, showindex=row, floatfmt=\".2f\", headers=header))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
