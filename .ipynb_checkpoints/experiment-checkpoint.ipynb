{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.4/importlib/_bootstrap.py:321: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pickle\n",
    "import threading\n",
    "import numpy as np\n",
    "from progressbar import *\n",
    "from models import Model\n",
    "from models.ExpGlm import ExpGlm\n",
    "from models.WblGlm import WblGlm\n",
    "from models.NpGlm import NpGlm\n",
    "from models.RayGlm import RayGlm\n",
    "from features.delicious.extraction import run as delicious_run\n",
    "from features.movielens.extraction import run as movielens_run\n",
    "from features.dblp.extraction import run as dblp_run\n",
    "from features.utils import timestamp_delta_generator\n",
    "from features.autoencoder import encode\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_squared_log_error, median_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(dist):\n",
    "    return {\n",
    "        'np': NpGlm(),\n",
    "        'wbl': WblGlm(),\n",
    "        'exp': ExpGlm(),\n",
    "        'ray': RayGlm(),\n",
    "#         'pow': PowGlm(),\n",
    "#         'gom': GomGlm()\n",
    "    }[dist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_c_index(T_true, T_pred, Y):\n",
    "    total_number_of_pairs = 0\n",
    "    number_of_correct_predictions = 0\n",
    "\n",
    "    for i in range(len(T_true)):\n",
    "        for j in range(len(T_true) - 1, i, -1):\n",
    "            if Y[i] != 0 or Y[j] != 0:  # if one or both of the samples are in observation window\n",
    "                total_number_of_pairs += 1\n",
    "                if T_true[i] > T_true[j] and T_pred[i] > T_pred[j]:\n",
    "                    number_of_correct_predictions += 1\n",
    "                if T_true[i] < T_true[j] and T_pred[i] < T_pred[j]:\n",
    "                    number_of_correct_predictions += 1\n",
    "                if T_true[i] == T_true[j] and T_pred[i] == T_pred[j]:\n",
    "                    number_of_correct_predictions += 1\n",
    "\n",
    "    return number_of_correct_predictions / total_number_of_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(X, Y, T, convert_to_month=False):\n",
    "    T = T.astype(np.float64)\n",
    "    if convert_to_month:\n",
    "        T /= timestamp_delta_generator(months=1)\n",
    "    T += np.random.rand(len(T)) * Y\n",
    "\n",
    "    index = np.argsort(T, axis=0).ravel()\n",
    "    X = X[index, :]\n",
    "    Y = Y[index]\n",
    "    T = T[index]\n",
    "\n",
    "    return X, Y, T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: Model, X_train: np.ndarray, Y_train: np.ndarray, T_train: np.ndarray, X_test: np.ndarray,\n",
    "             Y_test: np.ndarray, T_test: np.ndarray, acc_thresholds):\n",
    "    model.fit(X_train, Y_train, T_train)\n",
    "\n",
    "    # T_pred = model.mean(X_test)\n",
    "    T_pred = model.quantile(X_test, .5).ravel()\n",
    "    #     T_pred = np.fmin(T_pred, max(T_test))\n",
    "\n",
    "    c_index = generate_c_index(T_test, np.fmin(T_pred, max(T_test)), Y_test)\n",
    "\n",
    "    k = Y_test.sum()\n",
    "    # X_test = X_test[:k, :]\n",
    "    T_test = T_test[:k]\n",
    "    T_pred = T_pred[:k]\n",
    "\n",
    "    res = np.abs(T_pred - T_test)\n",
    "\n",
    "    distance = np.zeros((len(acc_thresholds)))\n",
    "    for i in range(len(acc_thresholds)):\n",
    "        distance[i] = (res <= acc_thresholds[i]).sum() / len(res)\n",
    "\n",
    "    #     ev = explained_variance_score(T_test, T_pred)\n",
    "    mae = mean_absolute_error(T_test, T_pred)\n",
    "    rmse = mean_squared_error(T_test, T_pred) ** .5\n",
    "    msle = mean_squared_log_error(T_test, T_pred)\n",
    "    mre = (res / T_test).mean()\n",
    "    mad = median_absolute_error(T_test, T_pred)\n",
    "    #     r2 = r2_score(T_test, T_pred)\n",
    "\n",
    "    return (mae, mre, rmse, msle, mad, c_index) + tuple(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(dists, X_stat, X, Y, T, cv, acc_thresholds):\n",
    "    threads = []\n",
    "    results = {dist+pos: [] for dist in dists for pos in ['', '_stat']}\n",
    "    k_fold = StratifiedKFold(n_splits=cv, shuffle=True)\n",
    "\n",
    "    widget = [Bar('=', '[', ']'), ' ', Percentage()]\n",
    "    bar = ProgressBar(maxval=cv*len(dists)*2, widgets=widget)\n",
    "    \n",
    "    for training_indices, test_indices in k_fold.split(X=X, y=Y):\n",
    "        X_stat_train = X_stat[training_indices, :]\n",
    "        X_train = X[training_indices, :]\n",
    "        Y_train = Y[training_indices]\n",
    "        T_train = T[training_indices]\n",
    "\n",
    "        X_stat_test = X_stat[test_indices, :]\n",
    "        X_test = X[test_indices, :]\n",
    "        Y_test = Y[test_indices]\n",
    "        T_test = T[test_indices]\n",
    "\n",
    "        def worker():\n",
    "            for dist in dists:\n",
    "                model = get_model(dist)\n",
    "                scores = evaluate(model, X_train, Y_train, T_train, X_test, Y_test, T_test, acc_thresholds)\n",
    "                results[dist].append(scores)\n",
    "                bar.update(bar.value+1)\n",
    "                scores_stat = evaluate(model, X_stat_train, Y_train, T_train, X_stat_test, Y_test, T_test, acc_thresholds)\n",
    "                results[dist+'_stat'].append(scores_stat)\n",
    "                bar.update(bar.value+1)\n",
    "\n",
    "        job = threading.Thread(target=worker)\n",
    "        threads.append(job)\n",
    "        \n",
    "    bar.start()\n",
    "\n",
    "    for t in threads:\n",
    "        t.start()\n",
    "    for t in threads:\n",
    "        t.join()\n",
    "    \n",
    "    bar.finish()\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_name(dist):\n",
    "    return {\n",
    "        'np': '\\\\npglm\\\\textsc{+LAE}',\n",
    "        'wbl': '\\\\textsc{Wbl-Glm+LAE}',\n",
    "        'exp': '\\\\textsc{Exp-Glm+LAE}',\n",
    "        'ray': '\\\\textsc{Ray-Glm+LAE}',\n",
    "        'gom': '\\\\textsc{Gom-Glm+LAE}',\n",
    "        'np_stat': '\\\\npglm',\n",
    "        'wbl_stat': '\\\\textsc{Wbl-Glm}',\n",
    "        'exp_stat': '\\\\textsc{Exp-Glm}',\n",
    "        'ray_stat': '\\\\textsc{Ray-Glm}',\n",
    "        'gom_stat': '\\\\textsc{Gom-Glm}',\n",
    "    }[dist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_list, Y_raw, T_raw = dblp_run(delta=1, observation_window=6, n_snapshots=9)\n",
    "# X_list, Y_raw, T_raw = delicious_run(delta=1, observation_window=6, n_snapshots=9)\n",
    "# X_list, Y_raw, T_raw = movielens_run(delta=1, observation_window=6, n_snapshots=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "limit = 4000\n",
    "if len(Y_raw) > limit:\n",
    "    X = np.stack(X_list, axis=1)  # X.shape = (n_samples, timesteps, n_features)\n",
    "    X, _, Y_raw, _, T_raw, _ = train_test_split(X, Y_raw, T_raw, train_size=limit, stratify=Y_raw, shuffle=True)\n",
    "    for i in range(len(X_list)):\n",
    "        X_list[i] = X[:,i,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "4000/4000 [==============================] - 3s 691us/step - loss: 0.4275\n",
      "Epoch 2/100\n",
      "4000/4000 [==============================] - 1s 131us/step - loss: 0.2660\n",
      "Epoch 3/100\n",
      "4000/4000 [==============================] - 1s 131us/step - loss: 0.2361\n",
      "Epoch 4/100\n",
      "4000/4000 [==============================] - 1s 138us/step - loss: 0.2307\n",
      "Epoch 5/100\n",
      "4000/4000 [==============================] - 1s 140us/step - loss: 0.2293\n",
      "Epoch 6/100\n",
      "4000/4000 [==============================] - 1s 141us/step - loss: 0.2288\n",
      "Epoch 7/100\n",
      "4000/4000 [==============================] - 1s 134us/step - loss: 0.2285\n",
      "Epoch 8/100\n",
      "4000/4000 [==============================] - 1s 139us/step - loss: 0.2282\n",
      "Epoch 9/100\n",
      "4000/4000 [==============================] - 1s 134us/step - loss: 0.2280\n",
      "Epoch 10/100\n",
      "4000/4000 [==============================] - 1s 132us/step - loss: 0.2278\n",
      "Epoch 11/100\n",
      "4000/4000 [==============================] - 1s 131us/step - loss: 0.2277\n",
      "Epoch 12/100\n",
      "4000/4000 [==============================] - 1s 135us/step - loss: 0.2276\n",
      "Epoch 13/100\n",
      "4000/4000 [==============================] - 1s 134us/step - loss: 0.2275\n",
      "Epoch 14/100\n",
      "4000/4000 [==============================] - 1s 131us/step - loss: 0.2274\n",
      "Epoch 15/100\n",
      "4000/4000 [==============================] - 1s 136us/step - loss: 0.2274\n",
      "Epoch 16/100\n",
      "4000/4000 [==============================] - 1s 142us/step - loss: 0.2273\n",
      "Epoch 17/100\n",
      "4000/4000 [==============================] - 1s 135us/step - loss: 0.2273\n",
      "Epoch 18/100\n",
      "4000/4000 [==============================] - 1s 132us/step - loss: 0.2272\n",
      "Epoch 19/100\n",
      "4000/4000 [==============================] - 1s 132us/step - loss: 0.2272\n",
      "Epoch 20/100\n",
      "4000/4000 [==============================] - 1s 136us/step - loss: 0.2272\n",
      "Epoch 21/100\n",
      "4000/4000 [==============================] - 1s 138us/step - loss: 0.2271\n",
      "Epoch 22/100\n",
      "4000/4000 [==============================] - 1s 128us/step - loss: 0.2271\n",
      "Epoch 23/100\n",
      "4000/4000 [==============================] - 1s 128us/step - loss: 0.2271\n",
      "Epoch 24/100\n",
      "4000/4000 [==============================] - 1s 136us/step - loss: 0.2271\n",
      "Epoch 25/100\n",
      "4000/4000 [==============================] - 1s 136us/step - loss: 0.2271\n",
      "Epoch 26/100\n",
      "4000/4000 [==============================] - 1s 135us/step - loss: 0.2270\n",
      "Epoch 27/100\n",
      "4000/4000 [==============================] - 1s 128us/step - loss: 0.2270\n",
      "Epoch 28/100\n",
      "4000/4000 [==============================] - 1s 134us/step - loss: 0.2270\n",
      "Epoch 29/100\n",
      "4000/4000 [==============================] - 1s 129us/step - loss: 0.2270\n",
      "Epoch 30/100\n",
      "4000/4000 [==============================] - 1s 131us/step - loss: 0.2270\n",
      "Epoch 31/100\n",
      "4000/4000 [==============================] - 1s 139us/step - loss: 0.2270\n",
      "Epoch 32/100\n",
      "4000/4000 [==============================] - 1s 134us/step - loss: 0.2270\n",
      "Epoch 33/100\n",
      "4000/4000 [==============================] - 1s 132us/step - loss: 0.2270\n",
      "Epoch 34/100\n",
      "4000/4000 [==============================] - 1s 141us/step - loss: 0.2269\n",
      "Epoch 35/100\n",
      "4000/4000 [==============================] - 1s 136us/step - loss: 0.2269\n",
      "Epoch 36/100\n",
      "4000/4000 [==============================] - 1s 131us/step - loss: 0.2269\n",
      "Epoch 37/100\n",
      "4000/4000 [==============================] - 1s 128us/step - loss: 0.2269\n",
      "Epoch 38/100\n",
      "4000/4000 [==============================] - 1s 133us/step - loss: 0.2269\n",
      "Epoch 39/100\n",
      "4000/4000 [==============================] - 1s 128us/step - loss: 0.2269\n",
      "Epoch 40/100\n",
      "4000/4000 [==============================] - 1s 129us/step - loss: 0.2269\n",
      "Epoch 41/100\n",
      "4000/4000 [==============================] - 1s 145us/step - loss: 0.2269\n",
      "Epoch 42/100\n",
      "4000/4000 [==============================] - 1s 148us/step - loss: 0.2268\n",
      "Epoch 43/100\n",
      "4000/4000 [==============================] - 1s 128us/step - loss: 0.2268\n",
      "Epoch 44/100\n",
      "4000/4000 [==============================] - 1s 137us/step - loss: 0.2268\n",
      "Epoch 45/100\n",
      "4000/4000 [==============================] - 1s 132us/step - loss: 0.2268\n",
      "Epoch 46/100\n",
      "4000/4000 [==============================] - 1s 132us/step - loss: 0.2268\n",
      "Epoch 47/100\n",
      "4000/4000 [==============================] - 1s 128us/step - loss: 0.2268\n",
      "Epoch 48/100\n",
      "4000/4000 [==============================] - 1s 133us/step - loss: 0.2268\n",
      "Epoch 49/100\n",
      "4000/4000 [==============================] - 1s 133us/step - loss: 0.2268\n",
      "Epoch 50/100\n",
      "4000/4000 [==============================] - 1s 135us/step - loss: 0.2267\n",
      "Epoch 51/100\n",
      "4000/4000 [==============================] - 1s 134us/step - loss: 0.2267\n",
      "Epoch 52/100\n",
      "4000/4000 [==============================] - 1s 129us/step - loss: 0.2267\n",
      "Epoch 53/100\n",
      "4000/4000 [==============================] - 1s 143us/step - loss: 0.2267\n",
      "Epoch 54/100\n",
      "4000/4000 [==============================] - 1s 131us/step - loss: 0.2267\n",
      "Epoch 55/100\n",
      "4000/4000 [==============================] - 1s 130us/step - loss: 0.2267\n",
      "Epoch 56/100\n",
      "4000/4000 [==============================] - 1s 136us/step - loss: 0.2267\n",
      "Epoch 57/100\n",
      "4000/4000 [==============================] - 1s 138us/step - loss: 0.2267\n",
      "Epoch 58/100\n",
      "4000/4000 [==============================] - 1s 136us/step - loss: 0.2266\n",
      "Epoch 59/100\n",
      "4000/4000 [==============================] - 1s 132us/step - loss: 0.2266\n",
      "Epoch 60/100\n",
      "4000/4000 [==============================] - 0s 121us/step - loss: 0.2266\n",
      "Epoch 61/100\n",
      "4000/4000 [==============================] - 1s 130us/step - loss: 0.2266\n",
      "Epoch 62/100\n",
      "4000/4000 [==============================] - 1s 129us/step - loss: 0.2266\n",
      "Epoch 63/100\n",
      "4000/4000 [==============================] - 1s 140us/step - loss: 0.2266\n",
      "Epoch 64/100\n",
      "4000/4000 [==============================] - 1s 135us/step - loss: 0.2266\n",
      "Epoch 65/100\n",
      "4000/4000 [==============================] - 1s 132us/step - loss: 0.2265\n",
      "Epoch 66/100\n",
      "4000/4000 [==============================] - 1s 140us/step - loss: 0.2265\n",
      "Epoch 67/100\n",
      "4000/4000 [==============================] - 1s 138us/step - loss: 0.2265\n",
      "Epoch 68/100\n",
      "4000/4000 [==============================] - 1s 135us/step - loss: 0.2265\n",
      "Epoch 69/100\n",
      "4000/4000 [==============================] - 1s 127us/step - loss: 0.2265\n",
      "Epoch 70/100\n",
      "4000/4000 [==============================] - 1s 133us/step - loss: 0.2265\n",
      "Epoch 71/100\n",
      "4000/4000 [==============================] - 1s 138us/step - loss: 0.2264\n",
      "Epoch 72/100\n",
      "4000/4000 [==============================] - 1s 144us/step - loss: 0.2264\n",
      "Epoch 73/100\n",
      "4000/4000 [==============================] - 1s 129us/step - loss: 0.2264\n",
      "Epoch 74/100\n",
      "4000/4000 [==============================] - 1s 138us/step - loss: 0.2264\n",
      "Epoch 75/100\n",
      "4000/4000 [==============================] - 1s 145us/step - loss: 0.2263\n",
      "Epoch 76/100\n",
      "4000/4000 [==============================] - 1s 138us/step - loss: 0.2263\n",
      "Epoch 77/100\n",
      "4000/4000 [==============================] - 1s 127us/step - loss: 0.2263\n",
      "Epoch 78/100\n",
      "4000/4000 [==============================] - 1s 129us/step - loss: 0.2263\n",
      "Epoch 79/100\n",
      "4000/4000 [==============================] - 1s 128us/step - loss: 0.2263\n",
      "Epoch 80/100\n",
      "4000/4000 [==============================] - 1s 135us/step - loss: 0.2262\n",
      "Epoch 81/100\n",
      "4000/4000 [==============================] - 1s 137us/step - loss: 0.2262\n",
      "Epoch 82/100\n",
      "4000/4000 [==============================] - 1s 140us/step - loss: 0.2262\n",
      "Epoch 83/100\n",
      "4000/4000 [==============================] - 1s 135us/step - loss: 0.2262\n",
      "Epoch 84/100\n",
      "4000/4000 [==============================] - 1s 126us/step - loss: 0.2261\n",
      "Epoch 85/100\n",
      "4000/4000 [==============================] - 1s 132us/step - loss: 0.2261\n",
      "Epoch 86/100\n",
      "4000/4000 [==============================] - 1s 133us/step - loss: 0.2261\n",
      "Epoch 87/100\n",
      "4000/4000 [==============================] - 1s 129us/step - loss: 0.2261\n",
      "Epoch 88/100\n",
      "4000/4000 [==============================] - 1s 142us/step - loss: 0.2260\n",
      "Epoch 89/100\n",
      "4000/4000 [==============================] - 1s 132us/step - loss: 0.2260\n",
      "Epoch 90/100\n",
      "4000/4000 [==============================] - 1s 135us/step - loss: 0.2260\n",
      "Epoch 91/100\n",
      "4000/4000 [==============================] - 1s 141us/step - loss: 0.2259\n",
      "Epoch 92/100\n",
      "4000/4000 [==============================] - 1s 128us/step - loss: 0.2259\n",
      "Epoch 93/100\n",
      "4000/4000 [==============================] - 1s 138us/step - loss: 0.2259\n",
      "Epoch 94/100\n",
      "4000/4000 [==============================] - 1s 132us/step - loss: 0.2258\n",
      "Epoch 95/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/4000 [==============================] - 1s 137us/step - loss: 0.2258\n",
      "Epoch 96/100\n",
      "4000/4000 [==============================] - 1s 138us/step - loss: 0.2258\n",
      "Epoch 97/100\n",
      "4000/4000 [==============================] - 1s 136us/step - loss: 0.2257\n",
      "Epoch 98/100\n",
      "4000/4000 [==============================] - 1s 144us/step - loss: 0.2257\n",
      "Epoch 99/100\n",
      "4000/4000 [==============================] - 1s 139us/step - loss: 0.2257\n",
      "Epoch 100/100\n",
      "4000/4000 [==============================] - 1s 132us/step - loss: 0.2256\n",
      "Autoencoder Training Loss: 0.2256\n"
     ]
    }
   ],
   "source": [
    "X_raw = encode(X_list, epochs=100, latent_factor=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[                                                                        ] N/A%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================================] 100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 149.40955424308777 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "X, Y, T = prepare_data(X_raw, Y_raw, T_raw)\n",
    "scaler = MinMaxScaler(copy=True)\n",
    "X_stat = scaler.fit_transform(X_list[0])\n",
    "\n",
    "dists = [\n",
    "    'np',\n",
    "    'wbl',\n",
    "    'exp',\n",
    "    'ray',\n",
    "    # 'gom'\n",
    "]\n",
    "\n",
    "print(len(T))\n",
    "\n",
    "results = cross_validate(dists, X_stat, X, Y, T, cv=5, acc_thresholds=[i/2 for i in range(1,7)])\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrrr}\n",
      "\\hline\n",
      "                      &   MAE &   MRE &   RMSE &   MSLE &   MDAE &   CI \\\\\n",
      "\\hline\n",
      " \\npglm\\textsc{+LAE}  &  1.99 &  0.95 &   2.43 &   0.30 &   1.73 & 0.55 \\\\\n",
      " \\textsc{Wbl-Glm+LAE} &  2.33 &  1.10 &   2.85 &   0.36 &   2.08 & 0.58 \\\\\n",
      " \\textsc{Exp-Glm+LAE} &  3.11 &  1.39 &   3.88 &   0.52 &   2.58 & 0.50 \\\\\n",
      " \\textsc{Ray-Glm+LAE} &  4.02 &  1.83 &   4.70 &   0.66 &   3.72 & 0.35 \\\\\n",
      " \\npglm               &  2.76 &  1.35 &   3.07 &   0.44 &   2.88 & 0.26 \\\\\n",
      " \\textsc{Wbl-Glm}     &  2.81 &  1.38 &   3.16 &   0.45 &   2.88 & 0.48 \\\\\n",
      " \\textsc{Exp-Glm}     &  3.28 &  1.57 &   3.70 &   0.53 &   3.30 & 0.14 \\\\\n",
      " \\textsc{Ray-Glm}     &  5.04 &  2.28 &   5.26 &   0.85 &   5.12 & 0.01 \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "table = []\n",
    "row = []\n",
    "header = ['MAE', 'MRE', 'RMSE', 'MSLE', 'MDAE', 'CI', 'ACC-1', 'ACC-2', 'ACC-3', 'ACC-4', 'ACC-5', 'ACC-6']\n",
    "for pos in ['', '_stat']:\n",
    "    for dist in dists:\n",
    "        row.append(get_name(dist+pos))\n",
    "        result = np.array(results[dist+pos])\n",
    "        mean = result.mean(axis=0)\n",
    "        table.append(mean[:6])\n",
    "print(tabulate(table, showindex=row, floatfmt=\".2f\", headers=header[:6], tablefmt='latex_booktabs'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
