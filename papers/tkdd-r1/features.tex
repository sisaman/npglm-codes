\section{Feature Extraction Framework}\label{sec:features}

In this section, we present our framework to extract features which is designed to have three major characteristics: First, it effectively considers different type of nodes and links available in a heterogeneous information network and regards their impact on the building time of the target relationship. Second, it takes the temporal dynamics of the network into account and leverages the network evolution history instead of simply aggregating it into a single snapshot. Finally, the extracted features are suitable for not only the link prediction problem, but also the generalized \emph{relationship prediction}. We will incorporate these features in the proposed non-parametric model in Section~\ref{sec:method} to solve the continuous-time relationship prediction problem.

\begin{figure}
	\definecolor{blue}{HTML}{84CECC}
	\definecolor{darkblue}{HTML}{375D81}
	\definecolor{green}{HTML}{3F7F47}
	\begin{chronology}[align=left, startyear=0,stopyear=200, width=\columnwidth, height=1pt, startdate=false, stopdate=false, arrowwidth=4pt, arrowheight=3pt]
		\footnotesize
		\chronoevent[date=false]{10}{$t_0$}
		\chronoevent[date=false]{40}{$t_0+\Delta$}
		\chronoevent[date=false]{70}{$t_0+2\Delta$}
		\chronoevent[date=false,mark=false]{100}{$\dots$}
		\chronoevent[date=false]{130}{$t_0+k\Delta$}
		\chronoevent[date=false]{190}{$t_1$}
		\chronoperiode[color=darkblue, startdate=false, bottomdepth=2pt, topheight=5pt, textdepth=8pt, stopdate=false]{10}{40}{$\Delta$}
		\chronoperiode[color=blue, startdate=false, bottomdepth=10pt, topheight=15pt, textdepth=-15pt, stopdate=false]{10}{129}{Feature Extraction Window $(\Phi=k\Delta)$}
		\chronoperiode[color=green, startdate=false, bottomdepth=10pt, topheight=15pt, textdepth=-15pt, stopdate=false]{131}{190}{Observation Window $(\Omega)$}
	\end{chronology}
	\caption{The evolutionary timeline of the network data.}
	\label{fig:timeline}
\end{figure}

\subsection{Data Preparation For Feature Extraction}
To solve the problem of continuous-time relationship prediction in dynamic networks, we need to pay attention to the temporal history of the network data from two different points of view. First, we have to mind the evolution history of the network for feature extraction, so that the extracted features reflect the changes made in the network over time. Second, we have to specify the exact relationship building time for each pair of nodes that have formed the target relationship. This is because our goal is to train a supervised model to predict a continuous variable, which in this case is the building time of the target relationship. Hence, for each sample pair of nodes, we need a feature vector $\mb{x}$, associated with a target variable $t$ that indicates the building time of the target relationship between them.

Suppose that we have observed a dynamic network $G^{\tau}$ recorded in the interval $t_0 <\tau\le t_1$. 
According to Fig.~\ref{fig:timeline}, we split this interval into two parts: the first part for extracting the feature $\mb{x}$, and the second for determining the target variable $t$. We refer to the first interval as \emph{Feature Extraction Window} whose length is denoted by $\Phi$, and the second as \emph{Observation Window}, whose length is denoted by $\Omega$. {\color{red}Now, based on the existence of the target relationship in the observation window, all the node pairs in the network fall within either one of the following three different groups:

\begin{enumerate}
	\item Node pairs that form the target relationship before the beginning of the observation window (in the feature extraction window).
	\item Node pairs that form the target relationship in the observation window for the first time (not existing before in the feature extraction window).
	\item Node pairs that do not form the target relationship (neither in the feature extraction window nor in the observation window).
\end{enumerate}

The node pairs in the 2nd and 3rd categories constitute our data samples, and will be used in the learning procedure to train the supervised model.} For such pairs, we extract their feature vector $\mb{x}$ using the history available in the feature extraction window. For each node pair in the 2nd category, we see that the target relationship between them has been created at a time like $t_r\in(t_0+\Phi,t_1]$. So we set $t=t_r-(t_0+\Phi)$ as the time it takes for the relationship to form since the beginning of the observation window. For these samples, we also set an auxiliary variable $y=1$ which indicates that we have \emph{observed} their exact building time. On the other hand, For node pairs in the 3rd category, we haven't seen their exact building time, but we know that it should be definitely after $t_1$. For such samples, that we call \emph{censored} samples, we set $t=t_1-(t_0+\Phi)$ that is equal to the length of the observation window $\Omega$, and set $y=0$ to indicate that the recorded time is in fact a lower bound on the true relationship building time. These type of samples are also of interest because their features will give us some information about their time falling after $t_1$. As a result, each data sample is associated with a triple $(\mb{x},y,t)$ representing its feature vector, observation status, and the time it takes for the target relationship to be formed, respectively.

%In Section \ref{sec:method}, we propose \npglm which is a supervised method to relate $x_l$ to $t_l$ by estimating $f_T(t_l\mid x_l)$ in a non-parametric fashion.

%Here is an toy example in a bibliographic network: Suppose that we have the data of the papers published between the years 1990 and 2010. For all papers, we have their authors, venue (where they are published), indexing terms, and their references. For each author pair, the goal is to predict by when one of them will cite another, if she has not done yet. To this end, we pick an intermediary year such as 2000 as pivot, and split the the data into two part. The paper that are published just before the year 2000 will belong to the feature extraction window, and the rest of the papers will fall within observation window. Now, for each pair of authors who did not cite each other in feature extraction window which 

\begin{table}[t]
	\centering
	\caption{Similarity Meta-Paths in Different Networks}
	\label{table:meta}
	\footnotesize
%	\setlength\tabcolsep{0pt}
	\begin{tabular} {c c l}
		\toprule
		Network & Meta-Path & Semantic Meaning \\
		\midrule
		\multirow{8}{*}{\rotatebox{90}{DBLP}} 
		&&\\
		& $A\rightarrow P\leftarrow A$ & Authors co-write a paper\\
		& $A\rightarrow P\leftarrow A\rightarrow P\leftarrow A$ & Authors have common co-author\\
		& $A\rightarrow P\leftarrow V\rightarrow P\leftarrow A$ & Authors publish in the same venue\\
		& $A\rightarrow P\rightarrow T\leftarrow P\leftarrow A$ & Authors use the same term\\
		& $A\rightarrow P\rightarrow P\leftarrow P\leftarrow A$ & Authors cite the same paper\\
		& $A\rightarrow P\leftarrow P\rightarrow P\leftarrow A$ & Authors are cited by the same paper\\
		&&\\
		\midrule
		\multirow{5}{*}{\rotatebox{90}{Delicious}} 
		&&\\
		& $U\leftrightarrow U\leftrightarrow U$ & Users have common contact\\
		& $U\rightarrow B\leftarrow U$ & Users post the same bookmark\\
		& $U\rightarrow B\rightarrow T\leftarrow B\leftarrow U$ & Users post bookmarks with the same tag\\
		&&\\
		\midrule
		\multirow{13}{*}{\rotatebox{90}{MovieLens}} 
		&&\\
		& $M\rightarrow A\leftarrow M$ & Movies share an actor\\
		& $M\rightarrow C\leftarrow M$ & Movies belong to the same country\\
		& $M\rightarrow D\leftarrow M$ & Movies have the same director\\
		& $M\rightarrow G\leftarrow M$ & Movies have the same genre\\
		& $M\rightarrow T\leftarrow M$ & Movies have the same tag\\
%		\cmidrule{2-3}
		& $U\rightarrow M\leftarrow U$ & Users rate common movie\\
		& $U\rightarrow M\rightarrow A\leftarrow M\leftarrow U$ & Users rate movies sharing an actor\\
		& $U\rightarrow M\rightarrow C\leftarrow M\leftarrow U$ & Users rate movies from the same country\\
		& $U\rightarrow M\rightarrow D\leftarrow M\leftarrow U$ & Users rate movies of the same director\\
		& $U\rightarrow M\rightarrow G\leftarrow M\leftarrow U$ & Users rate movies with the same genre\\
		& $U\rightarrow M\rightarrow T\leftarrow M\leftarrow U$ & Users rate movies with the same tag\\
		&&\\
		\bottomrule
	\end{tabular}
\end{table}

\subsection{Dynamic Feature Extraction}
In this part, we describe how to utilize the temporal history of the network in the feature extraction window in order to extract features for continuous-time relationship prediction problem. We first begin with the meta-path-based feature set for heterogeneous information networks, and then incorporate these features into a \emph{recurrent neural network based autoencoder} to exploit the temporal dynamics of the network as well. Hereby, we begin by defining the concept of meta-path \cite{sun2011pathsim}:

\begin{definition}[Meta-Path]
	In a heterogeneous information network, a meta-path is a directed path following the graph of the network schema to describe the general relations that can be derived from the network. Formally speaking, given a network schema $\mc{S}_G=(\mc{V}, \mc{E})$, the sequence $\nu_1\xrightarrow{\varepsilon_1}\nu_2\xrightarrow{\varepsilon_2}\dots\nu_{k-1}\xrightarrow{\varepsilon_{k-1}}\nu_k$ is a meta-path defined on $S_G$ where $\nu_i\in \mc{V}$ and $\varepsilon_i\in \mc{E}$.
\end{definition} 

Meta-paths are commonly used in heterogeneous information networks to describe multi-typed relations that have concrete semantic meanings. For example, in the bibliographic network whose schema is shown in Fig.~\ref{fig:schema:dblp}, we can define the co-authorship relation by the following meta-path:
\[Author\xrightarrow{write}Paper\xleftarrow{write}Author\]
or simply by $A\rightarrow P\leftarrow A$. Another example is the author citation relation, which in this paper is used as the target relation for DBLP network. It can be specified as:
\[Author\xrightarrow{write}Paper\xrightarrow{cite}Paper\xleftarrow{write}Author\]
abbreviated as $A\rightarrow P\rightarrow P\leftarrow A$.

{\color{red} We can extend the concept of heterogeneous adjacency matrix, which is used to indicate relationship between nodes of different types, to \emph{meta-path adjacency matrix}, which we will use to indicate the number of path instances between two nodes of (possibly) different types:

\begin{definition}{(Meta-path Adjacency Matrix)}
Given a heterogeneous network $G$ with schema $\mc{S}_G=(\mc{V}, \mc{E})$, and the meta-path $\nu_1\xrightarrow{\varepsilon_1}\nu_2\xrightarrow{\varepsilon_2}\dots\nu_{k-1}\xrightarrow{\varepsilon_{k-1}}\nu_k$ defined over $\mc{S}_G$ denoting the relation between node types $\nu_i,\nu_j\in\mc{V}$, the meta-path adjacency matrix $M_{\Psi}$ is defined as:
\[M_\Psi=\prod_{i=1}^{k-1}M_{\varepsilon_i}\]
which indicates the number of path instances between any node pair $u\in\nu_1$ and $v\in\nu_k$ following the meta-path $\Psi$. The time-aware counterpart of meta-path adjacency matrix is defined analogously using time-aware heterogeneous adjacency matrix.
\end{definition}
}

Among the possible meta-paths that can be defined on a network schema, there are some that capture the similarity between two nodes. For example, the co-authorship meta-path $A\rightarrow P\leftarrow A$ in a bibliographic network creates a sense of similarity between two \emph{Author} nodes. These type of meta-paths, called \emph{similarity meta-paths}, are widely used to define topological features for link prediction problem in heterogeneous networks \cite{sun2011co, zhang2014meta, 7752228}. Table~\ref{table:meta} presents a number of similarity meta-paths that can be defined on DBLP, Delicious, and MovieLens networks to capture the heterogeneous similarity between different node types.

The concept of similarity meta-paths can be extended to define heterogeneous features suitable for relationship prediction problem, where we have a target relation. Here we follow the same approach as in \cite{sun2012will} which suggests the following three meta-path-based building blocks to describe features for relationship prediction problem, given a target relation between two nodes of type $A$ and $B$:
\begin{enumerate}
	\small
	\item $A\xrsquigarrow{similarity}A\xrsquigarrow{target}B$
	\item $A\xrsquigarrow{target}B\xrsquigarrow{similarity}B$
	\item $A\xrsquigarrow{relation}C\xrsquigarrow{relation}B$
\end{enumerate}
where $\rightsquigarrow$ denotes a meta-path, with labels \emph{similarity} and \emph{target} denoting a similarity meta-path and the target relation, respectively. The \emph{relation} label denotes an arbitrary meta-path relating two nodes of possibly different types. The first block tells that there are some nodes of type $A$ similar to a single node of the same type that has made the target relationship with a node of type $B$. Therefore, those similar nodes may also form the target relation with the type $B$ node. An analogous intuition is behind the second block. For the third, it says that some nodes of type $A$ are in relation with some type $C$ nodes, which are themselves in relation with some nodes of type $B$. Hence, it is likely that type $A$ nodes form some relationships, such as the target relationship, with type $B$ nodes. {\color{red}We refer to the meta-paths that are created using these three blocks as \emph{feature meta-paths}.}

As an example in DBLP bibliographic network, for the target relation we use $A\rightarrow P\rightarrow P\leftarrow A$ as a meta-path denoting the author citation relation. In Addition, Paper-cite-Author ($P\rightarrow P\rightarrow A$) and Author-cite-Paper ($A\rightarrow P\rightarrow P$) are also used as the arbitrary relations, and the similarity meta-paths for DBLP network from Table~\ref{table:meta} are used to define the features for author citation relationship prediction.

After specifying feature meta-paths, we need a method to quantify them as numeric features. Due to the dynamicity of the network, different links are emerging and vanishing from the network over time. Therefore, the quantifying method must handle this dynamicity. Here, we formally define \emph{Time-Aware Meta-Path-based Features}:

\begin{definition}[Time-Aware Meta-Path-based Feature]
	Suppose that we are given a dynamic heterogeneous network $G^{\tau}$ along with its network schema $\mc{S}_G=(\mc{V}, \mc{E})$, and a target Relation $A\rightsquigarrow B$. {\color{red}For a given pair of nodes $a\in A$ and $b\in B$, and a feature meta-path $\Psi=A\xrightarrow{\varepsilon_1}\nu_1\xrightarrow{\varepsilon_2}\dots\nu_{n-1}\xrightarrow{\varepsilon_{n}}B$ defined on $\mc{S}_G$, the time-aware meta-path-based feature at the timestamp $\tau$ is the number of path instances between $a$ and $b$ following $\Psi$:
	\begin{equation*}
		f_{\Psi}^\tau(a,b)=M^\tau_{\Psi}[a,b]
	\end{equation*}}
\end{definition}

This way, for any pair of node, we can quantify the number of path instances of a particular meta-path at any specific timestamp $\tau$. {\color{red}Although this quantification requires matrix multiplication, it can be done efficiently due to the following reasons:
\begin{enumerate}
\item The heterogeneous adjacency matrices are highly sparse, thus for calculating meta-path adjacency matrices, we can hugely reduce the time complexity of each single matrix multiplication using fast sparse matrix multiplication algorithms, such as \cite{horowitz1983fundamentals}.
\item The process of calculating the meta-path adjacency matrices is highly parallelizable, as the corresponding meta-paths decouples into simpler similarity meta-paths, which themselves decouple further into link types. Therefore, we can calculate the adjacency matrix of different similarity meta-paths in parallel, and then multiply them together to get the feature meta-path adjacency matrices.
\item Due to the similarity meta-paths sharing common sub-paths, computation time for the similarity meta-paths can also be saved using dynamic programming to avoid recalculating previously computed products. For example, for the DBLP dataset, if the target relation is $A\rightarrow P\rightarrow P\leftarrow A$, then using the similarity meta-paths shown in the Table~\ref{table:meta}, the path $A\rightarrow P\rightarrow P$ will appear in all the following feature meta-paths:
\[A\rightarrow P\rightarrow P\leftarrow P\leftarrow A\]
\[A\rightarrow P\rightarrow P\rightarrow P\leftarrow A\]
\[A\rightarrow P\rightarrow P\leftarrow A\]
Therefore, we can calculate $M_{A\rightarrow P\rightarrow P}$ once and then reuse it in the calculation of the adjacency matrices of the above meta-paths.

\item Finally, the symmetry of the similarity meta-paths further reduces the number of products, because we can calculate the matrix corresponding to half of the path, and then multiply the resulting matrix by its transpose. For instance, the adjacency matrix of the similarity meta-path $A\rightarrow P\leftarrow V\rightarrow P\leftarrow A$ can be calculated as $X\cdot X^T$ where $X=M_{\text{write}}\cdot M_{\text{publish}}$, reducing the number of multiplications from three to two.
\end{enumerate}
}

So far we proposed a method to calculate the time-aware meta-path-based features, which is the number of path instances of a particular meta-path at the timestamp $\tau$. If we set this timestamp to the end of the feature extraction window, it is as though we are aggregating the whole network into a single snapshot observed at time $t_0+\Phi$. In order to avoid such an aggregation, we divide the feature extraction window into a sequence of $k$ contiguous intervals of a constant size $\Delta$, as shown in Fig.~\ref{fig:timeline}. By doing so, we intend to extract time-aware features in each sub-window that results in a multivariate time series containing the information about the temporal evolution of the topological features between any pair of nodes. With this in mind, we define \emph{Dynamic Meta-Path-based Time Series} as follows:

\begin{definition}[Dynamic Meta-Path-based Time Series]
	Suppose that we are given a dynamic heterogeneous network $G^{\tau}$ observed in a feature extraction window of size $\Phi$ ($t_0<\tau \le t_0+\Phi$), along with its network schema $\mc{S}_G=(\mc{V}, \mc{E})$ and a target relation $A\rightsquigarrow B$. Also suppose that the feature extraction window is divided into $k$ fragments of size $\Delta$. For a given pair of nodes $a\in A$ and $b\in B$ in $G^{t_0+\Phi}$, and a meta-path $\Psi$ defined on $\mc{S}_G$, the dynamic meta-path-based time series of $(a,b)$ is calculated as:
	\begin{equation*}
		x_{\Psi}^i(a,b)=f_{\Psi}^{t_0+i\Delta}(a,b) - f_{\Psi}^{t_0+(i-1)\Delta}(a,b)\quad\quad i=1\dots k
	\end{equation*}
\end{definition}

For each feature meta-path designed using the triple building blocks described before, we get a unique time series. For each time step, we put the corresponding values from all time series into a vector. Consequently, we get a multivariate time series where each time step is vector-valued. {\color{red}For example, if we have $d$ feature meta-paths $\Psi_1$ to $\Psi_d$, then each time step of the resulting time series for any node pair $(a,b)$ will become:
\[\mb{x}^i_{a,b}=[x_{\Psi_1}^i(a,b),\dots,x_{\Psi_d}^i(a,b)]^T,\quad i=1\dots k\]
We refer to this vector-valued time series as \emph{Multivariate Meta-Path-based Time Series}.} Such multivariate time series reflect how topological features change between two nodes across different snapshots of the network. Based on the level of the network dynamicity, it can capture increasing/decreasing trends or even periodic/re-occurring patterns.

Now it's time to convert the multivariate meta-path-based time series into a single feature vector so that we can use it as the input to our non-parametric model that will be discussed in the next section. A trivial solution would be to stack all the vector-valued time steps of the multivariate time series into a single vector. However, this approach will result in a very high dimensional vector as the number of time steps increases, and can lead to difficulties in the learning procedure due to the curse of dimensionality. This is in contrast with our expectation that more time steps would bring more information about the history of the network and should result in a better prediction model. To overcome this problem, we combine the power of recurrent neural networks, especially Long Short Term Memory (LSTM) units \cite{hochreiter1997long}, which have proven to be very successful in handling time series and sequential data, with Autoencoders \cite{bengio2009learning}, which are widely used to learn alternative representations of the data such that the learned representation can reconstruct the original input. {\color{red}Our goal is to transform the multivariate meta-path-based time series into a compact vector representation such that the resulting vector holds as much information from the original multivariate time series as possible.}

\begin{figure}
	\centering
	\footnotesize
	\tikzstyle{block} = [rectangle,draw=black,minimum width=0.5cm, minimum height=0.25cm]
	\tikzstyle{arrow} = [thick,->,>=stealth]
	\tikzstyle{label} = [rectangle]
	\begin{tikzpicture}
	\node[block] (e1) at (0,0) {};
	\node[block] (e2) at (1,0) {};
	\node[block,draw=none] (ed) at (2,0) {$\dots$};
	\node[block] (ek) at (3,0) {};
	
	\node[block] (dk) at (4,0) {};
	\node[block] (dk1) at (5,0) {};
	\node[block,draw=none] (dd) at (6,0) {$\dots$};
	\node[block] (d1) at (7,0) {};
	
	\node[label] (ie1) at (0,-1) {${x}^1$};
	\node[label] (ie2) at (1,-1) {${x}^2$};
	\node[label] (iek) at (3,-1) {${x}^k$};
	
	\node[label] (idk) at (4,-1) {$\mb{x}$};
	\node[label] (idk1) at (5,-1) {$\mb{x}$};
	\node[label] (id1) at (7,-1) {$\mb{x}$};
	
	\node[label] (oek) at (3,1) {$\mb{x}$};
	\node[label] (odk) at (4,1) {${x}^k$};
	\node[label] (odk1) at (5,1) {${x}^{k-1}$};
	\node[label] (od1) at (7,1) {${x}^1$};
	
	\draw [arrow] (ie1) -- (e1);
	\draw [arrow] (ie2) -- (e2);
	\draw [arrow] (iek) -- (ek);
	
	\draw [arrow] (idk) -- (dk);
	\draw [arrow] (idk1) -- (dk1);
	\draw [arrow] (id1) -- (d1);
	
	\draw [arrow] (ek) -- (oek);
	\draw [arrow] (dk) -- (odk);
	\draw [arrow] (dk1) -- (odk1);
	\draw [arrow] (d1) -- (od1);
	
	\draw [arrow] (e1) -- (e2);
	\draw [arrow] (e2) -- (ed);
	\draw [arrow] (ed) -- (ek);
	\draw [arrow] (ek) -- (dk);
	\draw [arrow] (dk) -- (dk1);
	\draw [arrow] (dk1) -- (dd);
	\draw [arrow] (dd) -- (d1);
	
	\end{tikzpicture}
	\caption{The architecture of the LSTM Autoencoder used for dynamic feature extraction. The first $k$ steps depicts the manner of the working of the encoder LSTM, while the second $k$ steps describes the decoder LSTM. The output of the $k^{\text{th}}$ stage is used as the feature vector $\mb{x}$, which is fed into the decoder $k$ times to produce the input sequence in the reversed order.}
	\label{fig:autoencoder}
\end{figure}

{\color{red}
Inspired by the work of Dai and Le on semi-supervised sequence learning \cite{dai2015semi}, we design an autoencoder that learns how to take a multivariate time series as input and compress it into a latent vector representation. The architecture of such autoencoder is illustrated in Fig.~\ref{fig:autoencoder}. The autoencoder consists of two components: (1) the encoder, which takes the input data and transforms it into a latent representation; and (2) the decoder, which takes the encoded representation and transforms it back to the input space. The autoencoder is trained in such a way that it can reconstruct the original input data. 

As the purpose of using the autoencoder in this paper is to compress multivariate time series, instead of using simple feed-forward neural networks, both encoder and decoder are built using LSTMs. The input to the encoder LSTM is a multivariate time series of length $k$. The encoder accepts the vector-valued time steps of the input multivariate time series sequentially. After receiving the $k^{\text{th}}$ time step, the output of the encoder LSTM will be the compressed feature vector that we will use as the input to \npglm method. In order to train the encoder to learn how to compress the input time series, it is matched with a decoder LSTM. The decoder LSTM receives $k$ copies of the compressed feature vector one after another, and with a proper loss function (such as mean squared error) it is forced to reconstruct the original multivariate time series in reverse order. Reversing the output sequence will make the optimization of the model easier, since it causes the decoder to revert back the changes made by the encoder to the input sequence.}

The benefits of using the LSTM autoencoder is three-fold: (1) since the autoencoder can reconstruct the original time series, which reflects the temporal dynamics of the network, we get minimum information loss in the compressed feature vector; (2) as we can set the dimensionality of the compressed feature vector to any desired value, we can evade the curse of dimensionality; {\color{red}and (3) due to the inherent dynamicity of recurrent neural networks and LSTMs, when we receive $k+1$st snapshot of the network, we can easily fine-tune the previous autoencoder that was learned with $k$ snapshots to consider the new snapshot as well, instead of repeating the whole learning procedure from scratch.}

{\color{red}
To conclude this section, we quickly review the whole procedure of processing the network data, training the autoencoder, and assembling a training dataset for the supervised model to predict the building time of a particular target relation:
\begin{enumerate}%[label=(\roman*)]
	\item The network evolution timeline is split into the feature extraction window and the observation window.
	\item Those node pairs that have either formed the target relationship in the observation window (observed samples) or have not formed the target relationship at all (censored samples) are selected as sample node pairs.
	\item By extracting feature meta-paths based on the target relation and similarity meta-paths, a multivariate time series can be obtained for each sample node pair. Thus if we have $N$ sample node pairs, we will have a dataset of $N$ multivariate time series.
	\item The LSTM autoencoder is trained using the dataset of $N$ multivariate time series to learn how to compress time series into feature vector.
	\item For each sample node pair, the corresponding multivariate time series is compressed into a feature vector $\mb{x}$ using the learned encoder LSTM. 
	\item For each observed node pair, the feature vector $\mb{x}$ is labeled with $y=1$ and associates with the variable $t$ denoting the time it takes for the node pair to form the target relationship. For censored node pairs, $y$ is set to zero and $t$ becomes equal to the size of the observation window.
	\item Finally, we will have a dataset of the form $\{\mb{x},y,t\}_i,\ i=1\dots N$ that will be used to train the supervised model.
\end{enumerate}}

We explain our proposed non-parametric model in the next section that takes the learned representation as the feature vector $\mb{x}$ and attempts to predict the corresponding event time $t$. 


