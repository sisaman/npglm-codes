\section{Experiments on Real Data}\label{sec:results}

We apply \npglm with the proposed feature set on a number of real-world datasets to evaluate its effectiveness and compare its performance in predicting the relationship building time vis-\`a-vis state of the art models. 

\subsection{Datasets}
\subsubsection{DBLP}
We use the DBLP bibliographic citation network, provided by \cite{tang2008aminer}, which has both attributes of dynamicity and heterogeneity. The network contains four types of objects: authors, papers, venues, and terms. The network schema of this dataset is depicted in Fig.~\ref{fig:schema:dblp}. Each paper is associated with a publication date, with a granularity of one year. Based on the publication venue of the papers, we limited the original DBLP dataset to those papers that are published in venues relative to the theoretical computer science. This resulted in having about 16k authors and 37k papers published from 1969 to 2016 in 38 venues. 

\subsubsection{Delicious}
Another dynamic and heterogeneous dataset we use in our experiments is the Delicious bookmarking dataset from \cite{Cantador:RecSys2011}, with a network schema presented in Fig.~\ref{fig:schema:delicious}. It contains three types of objects, namely users, bookmarks, and tags, whose numbers are about 1.7k, 31k, and 22k, respectively. The dataset includes bookmarking timestamps from May 2006 to October 2010.

\subsubsection{MovieLens}
The third heterogeneous dataset with dynamic characteristics has been extracted from MovieLens personalized movie recommendation website, provided by \cite{harper2015}. The dataset comprises seven types of objects, that are users, movies, tags, genres, actors, directors, and countries, as illustrated by the network schema in the Fig.~\ref{fig:schema:movielens}. It contains about 1.4k users and 5.6k movies, with user-movie rating timestamps ranging from September 1997 to January 2009.

The demographic statistics of all datasets are presented in Table~\ref{table:dataset}.


\begin{table}[t]
    \centering
    \caption{Demographic Statistics of Real-World Datasets}
    \label{table:dataset}
    \footnotesize
    \begin{tabular} {l l l l r l r}
        \toprule
        Dataset & Time Span & Entity & \multicolumn{4}{c}{Count}\\
        \midrule % In-table horizontal line
        \multirow{4}{*}{DBLP} & \multirow{4}{*}{From 1969 to 2016}
        & \multirow{2}{*}{Nodes}
        & $Author$ & 15,929 & $Venue$ & 38 \\ % Content row 1
        & & & $Paper$ & 37,077 & $Term$ & 12,028 \\ % Content row 2
        \cmidrule{3-7}
        & & \multirow{2}{*}{Links}
        & write & 100,797 & publish & 42,872 \\ % Content row 1
        & & & cite & 165,904 & mention & 284,156 \\ % Content row 2
        
        \midrule % In-table horizontal line
        \multirow{4}{*}{Delicious} & \multirow{4}{*}{From May 2006 to Oct 2010}
        & \multirow{2}{*}{Nodes}
        & $User$ & 1,714 & $Tag$ & 21,956 \\ % Content row 1
        & & & $Bookmark$ & 30,998 & & \\ % Content row 3
        \cmidrule{3-7}
        & & \multirow{2}{*}{Links}
        & contact & 15,329 & has-tag & 437,594 \\ % Content row 1
        & & & post & 437,594 & & \\ % Content row 2
        
        \midrule % In-table horizontal line
        \multirow{7}{*}{MovieLens} & \multirow{7}{*}{From Sep 1997 to Jan 2009}
        & \multirow{4}{*}{Nodes}
        & $User$ & 1,421 & $Genre$ & 19 \\ % Content row 1
        & & & $Movie$ & 5,660 & $Tag$ & 5,561 \\ % Content row 2
        & & & $Actor$ & 6,176 & $Country$ & 63 \\ % Content row 3
        & & & $Director$ & 2,401 & & \\ % Content row 3
        \cmidrule{3-7}
        & & \multirow{3}{*}{Links}
        & rate & 855,599 & has-genre & 20,810 \\ % Content row 1
        & & & play-in & 231,743 & has-tag & 47,958 \\ % Content row 2
        & & & direct & 10,156 & produced-in & 10,198 \\ % Content row 3
        \bottomrule % Bottom horizontal line
    \end{tabular}
\end{table}

\subsection{Experiment Settings}
\subsubsection{Comparison Methods}
To challenge the performance of \npglm, we use a number of baselines introduced in the following:

\begin{itemize}\color{red}
\item \emph{Generalized Linear Model (\textsc{Glm})}: This is the state-of-the-art method proposed in \cite{sun2012will}. We use the GLM-based framework with Exponential and Weibull distributions, denoted as \textsc{Exp-Glm} and \textsc{Wbl-Glm} used in \cite{sun2012will}.

\item \emph{Censored Regression Model (\textsc{Crm})}: This model, also called type II Tobit model, is designed to estimate linear relationships between variables when there is censoring in the dependent variable. In other words, it is an extension to the ordinary least squares linear regression for cencored data \cite{tobit}. The structural equation in this model is:
\[ t^*=\mb{w}^T\mb{x}+\epsilon \]
where $\epsilon$ is a normally distributed error term and $t^*$ is a latent variable which is observed within the observation window and censored otherwise. Accordingly, the observed $t$ is defined as:
\[ t=\begin{cases}
t^* & \text{if}\quad y=1 \\
\Omega & \text{if}\quad y=0\\
\end{cases} \]
The coefficient vector $\mb{w}$ is learned using maximum likelihood estimation (more details in \cite{amemiya1984tobit}).
\item \emph{Additive Regression Model (\textsc{Arm})}: This model is another regression method suggested by Aalen for censored data \cite{aalen1989linear}. Like \npglm, it specifies the intensity function, but instead of a multiplicative linear model, the Aalen's model is additive:
\[\lambda(t\mid \mb{x})=\sum_{i=0}^dw_i(t)x_i \]
The learning algorithm infers $\int_{0}^{t}w_i(t)dt$ instead of estimating individual $w_i$s. 
For more details about the learning algorithm, the reader can refer to \cite{hosmer2011applied}.

\end{itemize}

For all models, we consider the median of the distribution $f_T(t\mid\mb{x}_{test})$ as the predicted time for any test sample and then compare it to the ground truth time $t_{test}$.

To examine the effect of considering different feature extractors on the performance of the models, we use another dynamic feature extractor and a static one against the proposed LSTM Autoencoder:
\begin{itemize}
\item \emph{Exponential Smoothing}: This dynamic feature extractor previously used in \cite{hajibagheri2016leveraging} is an exponentially weighted moving average over the features extracted in all the snapshots of the network, which is calculated as:
\[\mb{f}^i=\begin{cases} 
\mb{x}^1, & \text{if}\quad i=1 \\
\alpha\mb{x}^i+(1-\alpha)\mb{f}^{i-1}, & \text{otherwise}
\end{cases}\]
where $\mb{f}^i$ is the smoothed feature after $i$th snapshot, $\mb{x}^i$ is the $i$th step of the dynamic meta-path-based time series, and $\alpha\in(0,1)$ is the smoothing factor. We then set $\mb{x}=\mb{f}^k$ as the final feature vector if we have $k$ snapshots in total.
\item \emph{Single Snapshot}: This static feature extractor considers the whole network as a single snapshot, neglecting its temporal dynamics. This feature extractor is equivalent to the one proposed in \cite{sun2012will}.
\end{itemize}

\subsubsection{Performance Measures}
We assess different methods using a number of evaluation metrics which are described in the following:
\begin{itemize}
\item Mean Absolute Error (MAE): This metric measures the expected absolute error between the predicted time values and the ground truth:
\[MAE(\mb{t},\hat{\mb{t}}) = \frac{1}{N}\sum_{i=1}^{N}\left|t_i-\hat{t}_i\right|\]
\item Mean Relative Error (MRE): This metric calculates the expected relative absolute error between the predicted time values and the ground truth:
\[MRE(\mb{t},\hat{\mb{t}}) = \frac{1}{N}\sum_{i=1}^{N}\left|\frac{t_i-\hat{t}_i}{t_i}\right|\]
\item Root Mean Squared Error (RMSE): This metric computes the root of the expected squared error between the predicted time values and the ground truth:
\[RMSE(\mb{t},\hat{\mb{t}}) = \sqrt{\frac{1}{N}\sum_{i=1}^{N}\left(t_i-\hat{t}_i\right)^2}\]
\item Mean Squared Logarithmic Error (MSLE): This measures the expected value of the squared logarithmic error between the predicted time values and the ground truth:
\[MSLE(\mb{t},\hat{\mb{t}}) = \frac{1}{N}\sum_{i=1}^{N}\left(\log{(1+t_i)}-log{(1+\hat{t}_i)}\right)^2\]
\item Median Absolute Error (MDAE): It is the median of the absolute errors between the predicted time values and the ground truth:
\[MDAE(\mb{t},\hat{\mb{t}}) = median(\left|t_1-\hat{t}_1\right|\dots\left|t_N-\hat{t}_N\right|)\]
\item Maximum Threshold Prediction Accuracy (ACC): This measures for what fraction of samples, a model have a lower absolute error than a given threshold:
\[ACC(\mb{t},\hat{\mb{t}})=\frac{1}{N}\sum_{i=1}^{N}\mb{1}\left(\left|t_i-\hat{t}_i\right| < threshold\right)\]
\item Concordance Index (CI): This metric is one of the most widely used performance measures for survival models that estimates how good a model performs at ranking predicted times \cite{harrell1982evaluating}. It can be seen as the fraction of all the sample pairs whose predicted timestamps are correctly ordered among all samples that can be ordered, and is considered as the generalization of the Area Under Receiver Operating Characteristic Curve (AUC) when we are dealing with censored data \cite{steck2008ranking}.
\end{itemize}

%We use 5-fold cross-validation and report the average results for all the experiments in this section. 
\subsubsection{Experiment Setup}


For DBLP dataset, we confine the data samples to those authors who have published more than 5 papers in the feature extraction window of each experiment. Following the triple building blocks described for feature extraction in Section~\ref{sec:features}, and using the similarity meta-paths in Table~\ref{table:meta}, we start the feature extraction process with 19 feature meta-paths. In all experiments, the author citation relation ($A\rightarrow P\rightarrow P\leftarrow A$) is chosen as the target relation. 
For the case of the Delicious dataset, we select user-user relation ($U\leftrightarrow U$) as the target relation, and design 6 feature meta-paths via the similarity meta-paths in Table~\ref{table:meta}.
Regarding the MovieLens dataset, we limit the actor list to the top three for each movie. To imply a notion of ``like'' relation between user and movie, we only consider ratings above 4 in the scale of 5. For this dataset, the target relation is set to user rate movie ($U\rightarrow M$), based on which, we design 11 final meta-paths. For the sake of convenience, we convert the scale of time differences from timestamp to month in Delicious and MovieLens datasets.

Except for parameter settings analysis (subsection~\ref{sec:param-analysis}) where we will analyze the effect of different parameters on the performance of different models, in the rest of the experiments in this section we set the length of the observation window $\Omega$ to 6 for all three datasets. For DBLP dataset, the number of snapshots $k$ is set to 6, while for the other two datasets we set $k=12$. We also fix the time difference between network snapshots $\Delta$ to 1 in all cases. These settings lead to having a feature extraction window of size $\Phi=6$ years for DBLP, and $\Phi=12$ months for Delicious and MovieLens. Accordingly, the number of labeled instances for DBLP, Delicious, and MovieLens are about 3.4K, 3.9K, and 7.8K, respectively. About half of the labeled samples are censored ones, which are picked uniformly at random among all the possible candidates.

We implemented the LSTM autoencoder using Keras deep learning library \cite{chollet2015keras}. We used mean square error loss function, linear activation function, and Adadelta optimizer \cite{zeiler2012adadelta} with default parameters. For all datasets, we set the dimension of the encoded feature as twice as the input dimension and trained the autoencoder in 50 epochs. For exponential smoothing feature extractor, the smoothing factor $\alpha$ were tuned to maximize the performance on the training dataset. For Np-Glm, the data samples were ordered according to their corresponding time variables, as the model needs the samples sorted by their recorded time. We use 5-fold cross-validation and report the average results for all the experiments in this section.

\subsection{Experiment Results}
In the rest of this section, we first assess how well different methods perform over various datasets and compare their performance based on different measures. Next, we discuss the efficiency of our proposed method by measuring and comparing its running time against the other baselines. Finally, we analyze the effect of different parameters and problem configurations on the performance of competitive methods.

\begin{table}[t]
    \centering
    \caption{Comprehensive Performance Comparison of Different Methods}
    \label{table:results}
    \scriptsize
    \begin{tabu} to \columnwidth {X[c] c X[l] X[r] X[r] X[r] X[r] X[r] X[r]}
        \toprule
        %\cmidrule(l){2-3} \cmidrule{5-7}
        Dataset & Feature &
        Model &  MAE &   MRE &   RMSE &   MSLE &   MDAE &  CI \\
        \midrule
        \multirow{15}{*}[-3em]{\rotatebox{90}{DBLP}}
        & \multirow{6}{*}{\shortstack{LSTM Autoencoder\\(Dynamic)}}
        & \npglm  &  $\bm{1.99}$ &  $\bm{0.95}$ &   $\bm{2.43}$ &   $\bm{0.30}$ &  $\bm{1.73}$ & $\bm{0.62}$ \\
        & & \textsc{Wbl-Glm} &  2.33 &  1.10 &   2.85 &   0.36 &   2.08 & 0.58 \\
        & & \textsc{Exp-Glm} &  3.11 &  1.39 &   3.88 &   0.52 &   2.58 & 0.50 \\
%         & & \textsc{Ray-Glm} &  4.02 &  1.83 &   4.70 &   0.66 &   3.72 & 0.35 \\
        & & \textsc{Crm} & 3.08 & 1.06 & 3.32 & 2.04 & 2.98 & 0.37 \\
        & & \textsc{Arm} & 2.95 & 1.33 & 4.48 & 0.48 & 1.48 & 0.56 \\
%        & & \textsc{Svr} & 2.61 & 1.32 & 2.90 & 0.42 & 2.80 & 0.58 \\
        
        \cmidrule{2-9}                                                                            
        & \multirow{6}{*}{\shortstack{Exp. Smoothing\\(Dynamic)}}
        & \npglm               &  2.15 &  1.07  &  2.54  &  0.32  &  1.98 & 0.53 \\
        & & \textsc{Wbl-Glm}     &  2.50 &  1.22 &   2.89  &  0.38  &  2.46 & 0.58 \\
        & & \textsc{Exp-Glm}     &  3.20 &  1.49  &  3.73  &  0.51  &  3.06&  0.45 \\
%         & & \textsc{Ray-Glm}     &  4.50  & 2.07 &   4.90   & 0.75   & 4.62 & 0.19 \\
        & & \textsc{Crm} & 2.55 & 0.97 & 3.05 & 1.58 & 2.11 & 0.55 \\
        & & \textsc{Arm} & 6.75 & 2.83 & 7.86 & 1.17 & 6.39 & 0.60 \\
%        & & \textsc{Svr} & 2.47 & 1.16 & 2.97 & 0.38 & 2.24 & 0.58 \\
        
        
        \cmidrule{2-9}                                                                            
        & \multirow{6}{*}{\shortstack{Single Snapshot\\(Static)}}                                                  
        & \npglm               &  2.76 &  1.35 &   3.07 &   0.44 &   2.88 & 0.50 \\
        & & \textsc{Wbl-Glm}     &  2.81 &  1.38 &   3.16 &   0.45 &   2.88 & 0.48 \\
        & & \textsc{Exp-Glm}     &  3.28 &  1.57 &   3.70 &   0.53 &   3.30 & 0.14 \\
%         & & \textsc{Ray-Glm}     &  5.04 &  2.28 &   5.26 &   0.85 &   5.12 & 0.01 \\
        & & \textsc{Crm} & 2.96 & 1.03 & 3.21 & 1.73 & 2.97 & 0.38 \\
        & & \textsc{Arm} & 3.89 & 1.76 & 5.45 & 0.66 & 2.11 & 0.46 \\
        
%        & & \textsc{Svr} &  4.02 &  1.83 &   4.70 &   0.66 &   3.72 & 0.35 \\
        
        \midrule
        \multirow{15}{*}[-3em]{\rotatebox{90}{Delicious}}
        & \multirow{6}{*}{\shortstack{LSTM Autoencoder\\(Dynamic)}}
        & \npglm  &  $\bm{2.10}$ &  $\bm{1.20}$ &   $\bm{2.55}$ &   $\bm{0.35}$ &   $\bm{2.05}$ & $\bm{0.70}$ \\
        & & \textsc{Wbl-Glm} &  2.37 &  1.31 &   2.89 &   0.40 &   2.16 & 0.57 \\
        & & \textsc{Exp-Glm} &  3.21 &  1.58 &   3.84 &   0.54 &   2.89 & 0.55 \\
%         & & \textsc{Ray-Glm} &  3.90 &  2.07 &   4.66 &   0.68 &   3.91 & 0.40 \\
        & & \textsc{Crm} & 6.38 & 3.10 & 6.55 & 1.33 & 6.87 & 0.43 \\
        & & \textsc{Arm} & 5.20 & 2.56 & 6.23 & 0.86 & 4.99 & 0.52 \\
%        & & \textsc{Svr} &  4.02 &  1.83 &   4.70 &   0.66 &   3.72 & 0.35 \\
        
        \cmidrule{2-9}                 
        & \multirow{6}{*}{\shortstack{Exp. Smoothing\\(Dynamic)}}                                                  
        & \npglm               &  2.25  & 1.36  &  2.74  &  0.40  &  2.11 & 0.66 \\
        & & \textsc{Wbl-Glm}     &  2.61  & 1.64  &  3.20 &   0.47   & 2.17 & 0.56 \\
        & & \textsc{Exp-Glm}     &  3.52  & 1.99  &  4.54  &  0.62  &  3.20  &0.39 \\
%         & & \textsc{Ray-Glm}     &  4.68 &  2.62 &   5.24   & 0.84 &   4.37  &0.19 \\
        & & \textsc{Crm} & 3.28 & 3.69 & 3.84 & 2.07 & 2.88 & 0.43 \\
        & & \textsc{Arm} & 6.36 & 3.24 & 7.80 & 1.09 & 6.72 & 0.56 \\
%        & & \textsc{Svr} &  4.02 &  1.83 &   4.70 &   0.66 &   3.72 & 0.35 \\
        
        \cmidrule{2-9}                 
        & \multirow{6}{*}{\shortstack{Single Snapshot\\(Static)}}                                                  
        & \npglm               &  2.33 &  1.46 &   2.80 &   0.41 &   2.17 & 0.61 \\
        & & \textsc{Wbl-Glm}     &  2.65 &  1.62 &   3.23 &   0.47 &   2.26 & 0.43 \\
        & & \textsc{Exp-Glm}     &  3.35 &  1.91 &   4.17 &   0.59 &   2.75 & 0.35 \\
%         & & \textsc{Ray-Glm}     &  4.81 &  2.61 &   5.27 &   0.85 &   4.28 & 0.12 \\
        & & \textsc{Crm} & 3.06 & 2.05 & 3.47 & 1.53 & 2.84 & 0.38 \\
        & & \textsc{Arm} & 5.79 & 2.76 & 6.69 & 1.16 & 5.89 & 0.37 \\
%        & & \textsc{Svr} &  4.02 &  1.83 &   4.70 &   0.66 &   3.72 & 0.35 \\
        
        \midrule
        \multirow{15}{*}[-3em]{\rotatebox{90}{MovieLens}}
        & \multirow{6}{*}{\shortstack{LSTM Autoencoder\\(Dynamic)}}
        & \npglm  &  $\bm{2.48}$ &  $\bm{3.08}$ &   $\bm{3.04}$ &   $\bm{0.55}$ &  $\bm{2.14}$ & $\bm{0.70}$ \\
        & & \textsc{Wbl-Glm} &  3.06 &  3.61 &   3.79 &   0.65 &   2.60 & 0.56 \\
        & & \textsc{Exp-Glm} &  3.79 &  2.70 &   4.60 &   0.78 &   3.48 & 0.45 \\
%         & & \textsc{Ray-Glm} &  4.98 &  3.58 &   5.63 &   1.05 &   4.83 & 0.33 \\
        & & \textsc{Crm} & 3.07 & 3.47 & 3.74 & 2.02 & 2.51 & 0.40 \\
        & & \textsc{Arm} & 5.53 & 5.63 & 7.41 & 1.12 & 3.80 & 0.53 \\
%        & & \textsc{Svr} &  4.02 &  1.83 &   4.70 &   0.66 &   3.72 & 0.35 \\
        
        \cmidrule{2-9}                                                                            
        & \multirow{6}{*}{\shortstack{Exp. Smoothing\\(Dynamic)}}                                                  
        & \npglm               &  2.69 &  3.35  &  3.18  &  0.59  &  2.61  &0.66 \\
        & & \textsc{Wbl-Glm}     &  3.09&   3.62 &   3.59 &   0.66 &   2.95 & 0.52 \\
        & & \textsc{Exp-Glm}     &  3.52 &  2.86  &  4.05  &  0.74  &  3.26 & 0.43 \\
%         & & \textsc{Ray-Glm}     &  5.01  & 3.88   & 5.46   & 1.06   & 4.99 & 0.22 \\
        & & \textsc{Crm} & 3.18 & 3.37 & 3.68 & 1.90 & 2.56 & 0.48 \\
        & & \textsc{Arm} & 9.39 & 8.60 & 10.06 & 1.83 & 9.26 & 0.52 \\
%        & & \textsc{Svr} &  4.02 &  1.83 &   4.70 &   0.66 &   3.72 & 0.35 \\
        
        \cmidrule{2-9}                                                                            
        & \multirow{6}{*}{\shortstack{Single Snapshot\\(Static)}}                                                  
        & \npglm               &  2.92 &  3.44 &   3.45 &   0.67 &   3.36 & 0.50 \\
        & & \textsc{Wbl-Glm}     &  2.99 &  3.52 &   3.51 &   0.69 &   3.37 & 0.49 \\
        & & \textsc{Exp-Glm}     &  3.42 &  2.89 &   3.86 &   0.78 &   3.82 & 0.49 \\
%         & & \textsc{Ray-Glm}     &  5.32 &  4.06 &   5.62 &   1.17 &   5.70 & 0.20 \\
        & & \textsc{Crm} & 3.14 & 3.48 & 3.63 & 2.20 & 3.55 & 0.35 \\
        & & \textsc{Arm} & 5.71 & 5.50 & 7.30 & 1.23 & 5.18 & 0.47 \\
%        & & \textsc{Svr} &  4.02 &  1.83 &   4.70 &   0.66 &   3.72 & 0.35 \\
        
        \bottomrule
    \end{tabu}
\end{table}

\begin{figure*}[t]
    \centering
    %\hfill
    \subfloat[DBLP]{
        \begin{tikzpicture}[trim axis left, trim axis right]
        \begin{axis}
        [
        tiny,
        width=0.35\columnwidth,
        height=4.5cm,
        legend pos=north west,
        legend image post style={scale=0.4},
        legend style={font=\tiny,nodes={scale=0.75, transform shape}},
        grid,
        y tick label style={
            /pgf/number format/.cd,
            fixed,
            fixed zerofill,
            precision=1,
            /tikz/.cd
        },
        xlabel=Absolute Error,
        ylabel=Prediction Accuracy,
        ylabel shift = -4 pt,
        %xticklabel style={rotate=90},
        ymax=1.1,
        xmin=0,
        xmax=3.5,
        ytick={0.1,0.2,...,0.9,1.0},
        xtick={0.5,1.0,...,3},
        %        restrict x to domain=0:900,
        legend entries={\npglm, \textsc{Wbl-Glm}, \textsc{Crm}, \textsc{Arm}},
        ]
        \addplot[color=purple,mark=square*,mark size=1.1,thick] table{results/db_np.txt};
        \addplot[color=cyan,mark=triangle*,mark size=1.1,thick] table{results/db_wbl.txt};
        \addplot[color=orange,mark=*,mark size=1.5,thick] table{results/db_tob.txt};
        \addplot[color=green,mark=diamond*,mark size=1.5,thick] table{results/db_aam.txt};
        \end{axis}
        \end{tikzpicture}
    }
    \hfil
    \subfloat[Delicious]{
        \begin{tikzpicture}[trim axis left, trim axis right]
        \begin{axis}
        [
        tiny,
        width=0.35\columnwidth,
        height=4.5cm,
        legend pos=north west,
        legend image post style={scale=0.4},
        legend style={font=\tiny,nodes={scale=0.75, transform shape}},
        grid,
        y tick label style={
            /pgf/number format/.cd,
            fixed,
            fixed zerofill,
            precision=1,
            /tikz/.cd
        },
        xlabel=Absolute Error,
        ylabel=Prediction Accuracy,
        ylabel shift = -4 pt,
        %xticklabel style={rotate=90},
        ymax=1.1,
        xmin=0,
        xmax=3.5,
        ytick={0.1,0.2,...,0.9,1.0},
        xtick={0.5,1.0,...,3},
        %        restrict x to domain=0:900,
        legend entries={\npglm, \textsc{Wbl-Glm}, \textsc{Crm}, \textsc{Arm}},
        ]
        \addplot[color=purple,mark=square*,mark size=1.1,thick] table{results/dl_np.txt};
        \addplot[color=cyan,mark=triangle*,mark size=1.1,thick] table{results/dl_wbl.txt};
        \addplot[color=orange,mark=*,mark size=1.5,thick] table{results/dl_tob.txt};
        \addplot[color=green,mark=diamond*,mark size=1.5,thick] table{results/dl_aam.txt};
        \end{axis}
        \end{tikzpicture}
    }
    \hfil
    \subfloat[MovieLens\label{fig:real:movielens}]{
        \begin{tikzpicture}[trim axis left, trim axis right]
        \begin{axis}
        [
        tiny,
        width=0.35\columnwidth,
        height=4.5cm,
        legend pos=north west,
        legend image post style={scale=0.4},
        legend style={font=\tiny,nodes={scale=0.75, transform shape}},
        grid,
        y tick label style={
            /pgf/number format/.cd,
            fixed,
            fixed zerofill,
            precision=1,
            /tikz/.cd
        },
        xlabel=Absolute Error,
        ylabel=Prediction Accuracy,
        ylabel shift = -4 pt,
        %xticklabel style={rotate=90},
        ymax=1.1,
        xmin=0,
        xmax=3.5,
        ytick={0.1,0.2,...,0.9,1.0},
        xtick={0.5,1.0,...,3},
        %        restrict x to domain=0:900,
        legend entries={\npglm, \textsc{Wbl-Glm}, \textsc{Crm}, \textsc{Arm}},
        ]
        \addplot[color=purple,mark=square*,mark size=1.1,thick] table{results/mv_np.txt};
        \addplot[color=cyan,mark=triangle*,mark size=1.1,thick] table{results/mv_wbl.txt};
        \addplot[color=orange,mark=*,mark size=1.5,thick] table{results/mv_tob.txt};
        \addplot[color=green,mark=diamond*,mark size=1.5,thick] table{results/mv_aam.txt};
        \end{axis}
        \end{tikzpicture}
    }
    \caption{Prediction accuracy of different methods vs the maximum tolerated absolute error on different datasets.}
    \label{fig:real}
\end{figure*}

\begin{figure}[t]
    \centering
    \subfloat[Mean Absolute Error]{
        \begin{tikzpicture}[trim axis left, trim axis right]
        \begin{axis}
        [
        tiny,
        width=0.4\columnwidth,
        height=4.5cm,
        legend pos=north east,
        legend style={font=\tiny,nodes={scale=0.75, transform shape}},
        grid,
        y tick label style={
            /pgf/number format/.cd,
            fixed,
            fixed zerofill,
            precision=1,
            /tikz/.cd
        },
        xlabel=\# Snapshots,
        ylabel=MAE,
        ylabel shift = -4 pt,
        %xticklabel style={rotate=90},
        %        ymax=1.1,
        xmin=0,
        xmax=21,
        %        ytick={0.1,0.2,...,0.9,1.0},
        xtick={3,6,...,18},
        %        restrict x to domain=0:900,
        legend entries={\npglm, \textsc{Wbl-Glm}},
        ]
        \addplot[color=purple,mark=square*,mark size=1.1,thick] table{results/dl_snap_mae_np.txt};
        \addplot[color=cyan,mark=triangle*,mark size=1.1,thick] table{results/dl_snap_mae_wbl.txt};
        \end{axis}
        \end{tikzpicture}
    }
    \hfil
    \subfloat[Concordance Index]{
        \begin{tikzpicture}[trim axis left, trim axis right]
        \begin{axis}
        [
        tiny,
        width=0.4\columnwidth,
        height=4.5cm,
        legend pos=north west,
        legend style={font=\tiny,nodes={scale=0.75, transform shape}},
        grid,
        y tick label style={
            /pgf/number format/.cd,
            fixed,
            fixed zerofill,
            precision=1,
            /tikz/.cd
        },
        xlabel=\# Snapshots,
        ylabel=CI,
        ylabel shift = -4 pt,
        %xticklabel style={rotate=90},
        %        ymax=1.1,
        xmin=0,
        %        xmax=3.5,
        %        ytick={0.1,0.2,...,0.9,1.0},
        xtick={3,6,...,18},
        %        restrict x to domain=0:900,
        legend entries={\npglm, \textsc{Wbl-Glm}},
        ]
        \addplot[color=purple,mark=square*,mark size=1.1,thick] table{results/dl_snap_ci_np.txt};
        \addplot[color=cyan,mark=triangle*,mark size=1.1,thick] table{results/dl_snap_ci_wbl.txt};
        \end{axis}
        \end{tikzpicture}
    }
    \caption{Effect of choosing different number of snapshots on performance of different methods using Delicious dataset.}
    \label{fig:snaps:delicious}
\end{figure}


\begin{figure}[t]
    \centering
    \subfloat[Mean Absolute Error]{
        \begin{tikzpicture}[trim axis left, trim axis right]
        \begin{axis}
        [
        tiny,
        width=0.4\columnwidth,
        height=4.5cm,
        legend pos=north east,
        legend style={font=\tiny,nodes={scale=0.75, transform shape}},
        grid,
        y tick label style={
            /pgf/number format/.cd,
            fixed,
            fixed zerofill,
            precision=1,
            /tikz/.cd
        },
        xlabel=\# Snapshots,
        ylabel=MAE,
        ylabel shift = -4 pt,
        %xticklabel style={rotate=90},
        %                ymax=18,ymin=10,
        xmin=0,
        %        xmax=3.5,
        %        ytick={0.1,0.2,...,0.9,1.0},
        xtick={3,6,...,18},
        %        restrict x to domain=0:900,
        legend entries={\npglm, \textsc{Wbl-Glm}},
        ]
        \addplot[color=purple,mark=square*,mark size=1.1,thick] table{results/mv_snap_mae_np.txt};
        \addplot[color=cyan,mark=triangle*,mark size=1.1,thick] table{results/mv_snap_mae_wbl.txt};
        \end{axis}
        \end{tikzpicture}
    }
    \hfil
    \subfloat[Concordance Index]{
        \begin{tikzpicture}[trim axis left, trim axis right]
        \begin{axis}
        [
        tiny,
        width=0.4\columnwidth,
        height=4.5cm,
        legend pos=north west,
        legend style={font=\tiny,nodes={scale=0.75, transform shape}},
        grid,
        y tick label style={
            /pgf/number format/.cd,
            fixed,
            fixed zerofill,
            precision=1,
            /tikz/.cd
        },
        xlabel=\# Snapshots,
        ylabel=CI,
        ylabel shift = -4 pt,
        %xticklabel style={rotate=90},
        ymax=0.9,ymin=0.2,
        xmin=0,
        %        xmax=3.5,
        %        ytick={0.1,0.2,...,0.9,1.0},
        xtick={3,6,...,18},
        %        restrict x to domain=0:900,
        legend entries={\npglm, \textsc{Wbl-Glm}},
        ]
        \addplot[color=purple,mark=square*,mark size=1.1,thick] table{results/mv_snap_ci_np.txt};
        \addplot[color=cyan,mark=triangle*,mark size=1.1,thick] table{results/mv_snap_ci_wbl.txt};
        \end{axis}
        \end{tikzpicture}
    }
    \caption{Effect of choosing different number of snapshots on performance of different methods using MovieLens dataset.}
    \label{fig:snaps:MovieLens}
\end{figure}

\begin{figure}[t]
    \centering
    \subfloat[Mean Absolute Error]{
        \begin{tikzpicture}[trim axis left, trim axis right]
        \begin{axis}
        [
        tiny,
        width=0.4\columnwidth,
        height=4.5cm,
        legend pos=north east,
        legend style={font=\tiny,nodes={scale=0.75, transform shape}},
        grid,
        y tick label style={
            /pgf/number format/.cd,
            fixed,
            fixed zerofill,
            precision=1,
            /tikz/.cd
        },
        xlabel=$\Delta$,
        ylabel=MAE,
        ylabel shift = -4 pt,
        %xticklabel style={rotate=90},
        %        ymax=1.1,
        xmin=0,
        xmax=3.5,
        %        ytick={0.1,0.2,...,0.9,1.0},
        xtick={0.5,1.0,...,3},
        %        restrict x to domain=0:900,
        legend entries={\npglm, \textsc{Wbl-Glm}},
        ]
        \addplot[color=purple,mark=square*,mark size=1.1,thick] table{results/delta_mae_np.txt};
        \addplot[color=cyan,mark=triangle*,mark size=1.1,thick] table{results/delta_mae_wbl.txt};
        \end{axis}
        \end{tikzpicture}
    }
    \hfil
    \subfloat[Concordance Index]{
        \begin{tikzpicture}[trim axis left, trim axis right]
        \begin{axis}
        [
        tiny,
        width=0.4\columnwidth,
        height=4.5cm,
        legend pos=north west,
        legend style={font=\tiny,nodes={scale=0.75, transform shape}},
        grid,
        y tick label style={
            /pgf/number format/.cd,
            fixed,
            fixed zerofill,
            precision=1,
            /tikz/.cd
        },
        xlabel=$\Delta$,
        ylabel=CI,
        ylabel shift = -4 pt,
        %xticklabel style={rotate=90},
        %        ymax=1.1,
        xmin=0,
        xmax=3.5,
        %        ytick={0.1,0.2,...,0.9,1.0},
        xtick={0.5,1.0,...,3},
        %        restrict x to domain=0:900,
        legend entries={\npglm, \textsc{Wbl-Glm}},
        ]
        \addplot[color=purple,mark=square*,mark size=1.1,thick] table{results/delta_ci_np.txt};
        \addplot[color=cyan,mark=triangle*,mark size=1.1,thick] table{results/delta_ci_wbl.txt};
        \end{axis}
        \end{tikzpicture}
    }
    \caption{Effect of choosing different values for $\Delta$ on performance of different methods using Delicious dataset.}
    \label{fig:delta:delicious}
\end{figure}


\begin{figure}[t]
    \centering
    \subfloat[Mean Absolute Error]{
        \begin{tikzpicture}[trim axis left, trim axis right]
        \begin{axis}
        [
        tiny,
        width=0.4\columnwidth,
        height=4.5cm,
        legend pos=north east,
        legend style={font=\tiny,nodes={scale=0.75, transform shape}},
        grid,
        y tick label style={
            /pgf/number format/.cd,
            fixed,
            fixed zerofill,
            precision=1,
            /tikz/.cd
        },
        xlabel=$\Delta$,
        ylabel=MAE,
        ylabel shift = -4 pt,
        %xticklabel style={rotate=90},
        ymax=18,ymin=10,
        xmin=0,
        xmax=3.5,
        %        ytick={0.1,0.2,...,0.9,1.0},
        xtick={0.5,1.0,...,3},
        %        restrict x to domain=0:900,
        legend entries={\npglm, \textsc{Wbl-Glm}},
        ]
        \addplot[color=purple,mark=square*,mark size=1.1,thick] table{results/mv_delta_mae_np.txt};
        \addplot[color=cyan,mark=triangle*,mark size=1.1,thick] table{results/mv_delta_mae_wbl.txt};
        \end{axis}
        \end{tikzpicture}
    }
    \hfil
    \subfloat[Concordance Index]{
        \begin{tikzpicture}[trim axis left, trim axis right]
        \begin{axis}
        [
        tiny,
        width=0.4\columnwidth,
        height=4.5cm,
        legend pos=north west,
        legend style={font=\tiny,nodes={scale=0.75, transform shape}},
        grid,
        y tick label style={
            /pgf/number format/.cd,
            fixed,
            fixed zerofill,
            precision=1,
            /tikz/.cd
        },
        xlabel=$\Delta$,
        ylabel=CI,
        ylabel shift = -4 pt,
        %xticklabel style={rotate=90},
        ymax=0.9,ymin=0.2,
        xmin=0,
        xmax=3.5,
        %        ytick={0.1,0.2,...,0.9,1.0},
        xtick={0.5,1.0,...,3},
        %        restrict x to domain=0:900,
        legend entries={\npglm, \textsc{Wbl-Glm}},
        ]
        \addplot[color=purple,mark=square*,mark size=1.1,thick] table{results/mv_delta_ci_np.txt};
        \addplot[color=cyan,mark=triangle*,mark size=1.1,thick] table{results/mv_delta_ci_wbl.txt};
        \end{axis}
        \end{tikzpicture}
    }
    \caption{Effect of choosing different values for $\Delta$ on performance of different methods using MovieLens dataset.}
    \label{fig:delta:MovieLens}
\end{figure}

\subsubsection{Comparative Performance Analysis}
In the first set of experiments, we evaluate the prediction power of different models combined with different feature extractors on DBLP, Delicious and MovieLens datasets. MAE, MRE, RMSE, MSLE, MDAE, and CI of all models using both dynamic and static feature sets has been shown in Table~\ref{table:results}. We see that in all three networks, \npglm with the LSTM Autoencoder feature set is superior to the other methods under all performance measures. For instance, our model \npglm can obtain an MAE of 1.99 for DBLP dataset, which is 15\% lower than the MAE obtained by its closest competitor, \textsc{Wbl-Glm}. As of CI, \npglm achieves 0.62 on DBLP, which is 7\% better than \textsc{Wbl-Glm}. On Delicious dataset, \npglm improves MAE and CI by 11\% and 23\%, respectively, relative to \textsc{Wbl-Glm}. Similarly, \npglm reduces MAE by 19\% and increases CI by 25\%. Comparable results hold for other performance measures as well. Accordingly, \textsc{Wbl-Glm}, which has two degrees of freedom, has shown a better performance compared to other models. That is while \npglm, as a non-parametric model with highly tunable shape, outperforms all the other ``less-flexible'' models by learning the true distribution of the data.

Moreover, it is evident from Table~\ref{table:results} that using the dynamic features learned with the LSTM autoencoder has boosted the performance of all models over different datasets, and has outperformed the other feature extractors. Based on the results presented in Table~\ref{table:results}, the alternative dynamic feature extractor, exponential smoothing, has performed better than the static single snapshot feature extractor, yet not better than the proposed LSTM Autoencoder. Comparing the LSTM Autoencoder with exponential smoothing feature extractor, over the DBLP dataset, the proposed feature extractor has achieved 7\% less MAE and 17\% more CI with \npglm. Over Delicious, \npglm with LSTM-based features reduces MAE by about 7\% and improves CI by 7\%. Finally, on the MovieLens dataset, combining LSTM Autoencoder and \npglm leads to an improvement of 8\% and 7\% under MAE and CI, respectively. The other models behave more or less similarly when they are combined with different feature extractors. This result clearly demonstrates that our feature extraction framework is performing well on capturing the temporal dynamics of the networks.

In the next experiment, we investigated the performance of different models using the LSTM autoencoder feature extraction framework under maximum threshold prediction accuracy. To evaluate the prediction accuracy of a model, we record the fraction of test samples for which the difference between their true times and predicted ones are lower than a given threshold, called \emph{tolerated error}. The results are plotted in Fig.~\ref{fig:real} where we varied the tolerated error in the range $\{0.5, 1.0, \dots, 3.0\}$. We can see from the figure that \npglm and \textsc{Wbl-Glm} perform comparably, yet \npglm outperforms \textsc{Wbl-Glm} in all cases. For example on MovieLens dataset (Fig.~\ref{fig:real:movielens}), \npglm can predict the relationship building time of all the test samples with 100\% accuracy by an error of 3 months, whereas for \textsc{Wbl-Glm}, this is reduced to 90\%. Similarly, on the Delicious dataset, \npglm with 3 months of tolerated error achieves around 80\% accuracy, which is about 12\% more than \textsc{Wbl-Glm}.


\subsubsection{Efficiency Analysis}
In this part, we analyze and compare the running time of \npglm and \textsc{Wbl-Glm} models utilizing different feature extractors, namely LSTM autoencoder, exponential smoothing, and single snapshot. All the algorithms were implemented in Python and were run on a Windows 10 PC with Intel Core i7 1.8 GHz CPU and 12GB of RAM. The full specification of the host machine is reported in Table~\ref{table:pc}. We measured the running time of all the methods during a complete training and test procedure, including feature extraction, learning, and inference. For exponential smoothing feature extractor, we included the time required for tuning the smoothing factor $\alpha$ using a separate validation set, while for LSTM based framework the training time of the autoencoder is counted toward total running time. Table~\ref{table:efficiency} presents the results over each of the DBLP, Delicious, and MovieLens datasets. Since a considerable amount of running time is spent on feature extraction, dynamic feature extraction frameworks require more time to process the network data as opposed to static single snapshot feature extractors. However, the proposed LSTM autoencoder performs comparably to exponential smoothing in terms of running time. Even though LSTM autoencoder is a bit slower than the other dynamic feature extractors, it demonstrates higher prediction performance compared to models utilizing exponential smoothing. For example, on MovieLens network with more than 20K nodes and 1 million links, \npglm with LSTM autoencoder requires less than four minutes to process the whole network, extract features and learn from about 6K labeled samples, and perform prediction for about 2K instances on a typical PC.


\begin{table}[t]
    \centering
    \caption{Comparison of Computational Time Measured in Seconds}
    \label{table:efficiency}
    \footnotesize
    \begin{tabu} to \columnwidth {c c c c c}
        \toprule
        %\cmidrule(l){2-3} \cmidrule{5-7}
        \multirow{2}{*}{Dataset} & \multirow{2}{*}{Model} &
        \multicolumn{3}{c}{Feature Extractor} \\
        \cmidrule{3-5}
        & & Single Snapshot & Exp. Smoothing & LSTM Autoencoder\\
        
        \midrule
        \multirow{2}{*}{DBLP} & \npglm  &  35.92 &  98.35 &   93.59\\
                              & \textsc{Wbl-Glm} &  36.01 &  74.34 &   79.83 \\
        
        \midrule
        \multirow{2}{*}{Delicious} & \npglm  &  2.01 & 110.67 &   128.43\\
        & \textsc{Wbl-Glm} &  1.89 &  97.44 &   123.53 \\
        
        \midrule
        \multirow{2}{*}{MovieLens} & \npglm  &  19.60 &  177.015 &   232.77\\
        & \textsc{Wbl-Glm} &  19.70 &  154.86 &   213.7\\
        \bottomrule
    \end{tabu}
\end{table}

\subsubsection{Parameter Setting Analysis}\label{sec:param-analysis}
The performance of different models is influenced by two parameters, the number of snapshots $k$, and the time difference between snapshots $\Delta$, as these parameters determine the length of the feature extraction window $\Phi$. In this set of experiments, we investigate how these parameters affect the performance of our model \npglm and its closest competitor \textsc{Wbl-Glm} over Delicious and MovieLens datasets using the proposed LSTM based feature extraction framework. 

Firstly, The effect of increasing the number of snapshots on achieved MAE and CI by \npglm and \textsc{Wbl-Glm} over Delicious and MovieLens datasets is illustrated in Fig.~\ref{fig:snaps:delicious} and Fig.~\ref{fig:snaps:MovieLens}, respectively. For both datasets, we set $\Delta=1.5$ and $\Omega=18$ and varied the number of snapshots in the range of 3 to 18. As we can see in both figures, increasing the number of snapshots results in lower prediction error and higher accuracy. This is due to the fact that as the number of snapshots grows, a longer history of the network is taken into account. Therefore, different models can benefit from more information about the temporal dynamics of the network given to them through the extracted feature vector.

Finally, the impact of choosing different values for $\Delta$ is analyzed on the performance of \npglm and \textsc{Wbl-Glm} in terms of MAE and CI. The results for Delicious and MovieLens datasets are depicted in Fig.~\ref{fig:delta:delicious} and Fig.~\ref{fig:delta:MovieLens}, respectively. In this experiment, the number of snapshots and observation window length are accordingly set to 6 and 24. Different values of $\Delta$ are selected from the set $\{0.5,1.0,\dots,3.0\}$. As illustrated in both figures, by increasing  $\Delta$ up to an extent, we witness that the performance of models improves gradually. That is because increasing the value of $\Delta$ leads to a wider feature extraction window. However, since the number of snapshots is constant, we see no performance improvement when the value of $\Delta$ becomes greater than a certain threshold. This is due to the fact that short-term temporal evolution of the network will be ignored when the value of $\Delta$ becomes too wide.
 



